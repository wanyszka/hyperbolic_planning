{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP Hyperbolic Embeddings: Hitting Time Conjecture\n",
    "\n",
    "This notebook explores hyperbolic embeddings for (state, goal) pairs in a 6-state MDP.\n",
    "\n",
    "**Conjecture:**\n",
    "- **Norm** (distance from origin) correlates **negatively** with **variance** in hitting times (low variance → high norm, high variance → low norm)\n",
    "- **Angular coordinate** correlates with **mean** hitting times\n",
    "\n",
    "## MDP Structure\n",
    "```\n",
    "State 1 (start)\n",
    "    |\n",
    "    |-- a11 (stochastic) --> 4 (p=0.5) or 5 (p=0.5) --> 6 (goal)\n",
    "    |\n",
    "    |-- a12 (deterministic) --> 2 --> (0.9 self-loop, 0.1 to 4) --> 6\n",
    "    |\n",
    "    |-- a13 (deterministic) --> 3 --> (0.9 self-loop, 0.1 to 5) --> 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hypll geoopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Import hypll library\n",
    "from hypll.manifolds.poincare_ball import PoincareBall, Curvature\n",
    "from hypll.tensors.manifold_tensor import ManifoldTensor\n",
    "from hypll.tensors import TangentTensor\n",
    "import hypll.nn as hnn\n",
    "\n",
    "# Setup\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMDP:\n",
    "    \"\"\"\n",
    "    6-state MDP with the following structure:\n",
    "    \n",
    "    States (0-indexed internally, displayed as 1-6):\n",
    "        0 (State 1): Start state, 3 actions available\n",
    "        1 (State 2): Self-loop (0.9) or to state 3 (0.1)\n",
    "        2 (State 3): Self-loop (0.9) or to state 4 (0.1)\n",
    "        3 (State 4): Deterministic to goal (state 5)\n",
    "        4 (State 5): Deterministic to goal (state 5)\n",
    "        5 (State 6): Goal (terminal)\n",
    "    \n",
    "    From state 0:\n",
    "        - a0: Stochastic -> state 3 (p=0.5) or state 4 (p=0.5)\n",
    "        - a1: Deterministic -> state 1\n",
    "        - a2: Deterministic -> state 2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_states = 6\n",
    "        self.start_state = 0  # State 1\n",
    "        self.goal_state = 5   # State 6\n",
    "        \n",
    "        # Number of actions per state\n",
    "        self.n_actions = {\n",
    "            0: 3,  # State 1: a11, a12, a13\n",
    "            1: 1,  # State 2: only one action (stochastic outcome)\n",
    "            2: 1,  # State 3: only one action (stochastic outcome)\n",
    "            3: 1,  # State 4: deterministic to goal\n",
    "            4: 1,  # State 5: deterministic to goal\n",
    "            5: 0,  # State 6: terminal\n",
    "        }\n",
    "        \n",
    "    def get_transitions(self, state, action):\n",
    "        \"\"\"\n",
    "        Get transition probabilities for (state, action) pair.\n",
    "        Returns list of (next_state, probability) tuples.\n",
    "        \"\"\"\n",
    "        if state == 0:  # State 1 (start)\n",
    "            if action == 0:  # a11: stochastic\n",
    "                return [(3, 0.5), (4, 0.5)]  # -> State 4 or 5\n",
    "            elif action == 1:  # a12: deterministic\n",
    "                return [(1, 1.0)]  # -> State 2\n",
    "            elif action == 2:  # a13: deterministic\n",
    "                return [(2, 1.0)]  # -> State 3\n",
    "        \n",
    "        elif state == 1:  # State 2\n",
    "            return [(1, 0.9), (3, 0.1)]  # Self-loop or -> State 4\n",
    "        \n",
    "        elif state == 2:  # State 3\n",
    "            return [(2, 0.9), (4, 0.1)]  # Self-loop or -> State 5\n",
    "        \n",
    "        elif state == 3:  # State 4\n",
    "            return [(5, 1.0)]  # -> Goal (State 6)\n",
    "        \n",
    "        elif state == 4:  # State 5\n",
    "            return [(5, 1.0)]  # -> Goal (State 6)\n",
    "        \n",
    "        elif state == 5:  # State 6 (goal)\n",
    "            return [(5, 1.0)]  # Stay at goal\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def step(self, state, action=None):\n",
    "        \"\"\"\n",
    "        Take a step from state with given action.\n",
    "        If action is None, sample uniformly from available actions.\n",
    "        Returns next_state.\n",
    "        \"\"\"\n",
    "        if state == self.goal_state:\n",
    "            return state\n",
    "        \n",
    "        if action is None:\n",
    "            n_actions = self.n_actions[state]\n",
    "            action = np.random.randint(0, max(1, n_actions))\n",
    "        \n",
    "        transitions = self.get_transitions(state, action)\n",
    "        \n",
    "        if len(transitions) == 1:\n",
    "            return transitions[0][0]\n",
    "        \n",
    "        # Sample according to probabilities\n",
    "        probs = [t[1] for t in transitions]\n",
    "        next_states = [t[0] for t in transitions]\n",
    "        return np.random.choice(next_states, p=probs)\n",
    "    \n",
    "    def state_name(self, state):\n",
    "        \"\"\"Return human-readable state name.\"\"\"\n",
    "        return f\"S{state + 1}\"\n",
    "\n",
    "\n",
    "# Test the MDP\n",
    "mdp = SimpleMDP()\n",
    "print(\"MDP Structure Test:\")\n",
    "print(f\"Start state: {mdp.state_name(mdp.start_state)}\")\n",
    "print(f\"Goal state: {mdp.state_name(mdp.goal_state)}\")\n",
    "print()\n",
    "for state in range(mdp.n_states):\n",
    "    print(f\"{mdp.state_name(state)}: {mdp.n_actions[state]} actions\")\n",
    "    for action in range(mdp.n_actions[state]):\n",
    "        transitions = mdp.get_transitions(state, action)\n",
    "        trans_str = \", \".join([f\"{mdp.state_name(s)} (p={p})\" for s, p in transitions])\n",
    "        print(f\"  a{action}: {trans_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mdp_trajectories(mdp, n_trajectories, max_length=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate trajectories from start to goal under uniform random policy.\n",
    "    \n",
    "    Args:\n",
    "        mdp: SimpleMDP instance\n",
    "        n_trajectories: Number of trajectories to generate\n",
    "        max_length: Maximum trajectory length\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        List of state sequences (lists of state indices)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    trajectories = []\n",
    "    \n",
    "    for _ in range(n_trajectories):\n",
    "        traj = [mdp.start_state]\n",
    "        state = mdp.start_state\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if state == mdp.goal_state:\n",
    "                break\n",
    "            \n",
    "            # Uniform random action selection\n",
    "            state = mdp.step(state, action=None)\n",
    "            traj.append(state)\n",
    "        \n",
    "        trajectories.append(traj)\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "\n",
    "# Generate trajectories\n",
    "trajectories = generate_mdp_trajectories(mdp, n_trajectories=5000, max_length=1000, seed=42)\n",
    "\n",
    "# Analyze trajectories\n",
    "lengths = [len(t) for t in trajectories]\n",
    "print(f\"Generated {len(trajectories)} trajectories\")\n",
    "print(f\"Length stats: mean={np.mean(lengths):.1f}, std={np.std(lengths):.1f}, min={min(lengths)}, max={max(lengths)}\")\n",
    "\n",
    "# Show sample trajectories\n",
    "print(\"\\nSample trajectories:\")\n",
    "for i in range(5):\n",
    "    traj_str = \" -> \".join([mdp.state_name(s) for s in trajectories[i][:15]])\n",
    "    if len(trajectories[i]) > 15:\n",
    "        traj_str += f\" ... ({len(trajectories[i])} states total)\"\n",
    "    print(f\"  {i+1}: {traj_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPContrastiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for contrastive learning on MDP trajectory intervals.\n",
    "    \n",
    "    For each sample:\n",
    "    - Anchor: [start_state, goal_state] interval from trajectory\n",
    "    - Positive: Subinterval [k, l] where anchor_i <= k <= l <= anchor_j\n",
    "    - Negatives: Intervals that are NOT subintervals of anchor\n",
    "    \n",
    "    States are normalized to [0, 1] by dividing by (n_states - 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trajectories, n_states=6, num_samples=10000, n_negatives=5, seed=42):\n",
    "        self.trajectories = trajectories\n",
    "        self.n_states = n_states\n",
    "        self.num_samples = num_samples\n",
    "        self.n_negatives = n_negatives\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        # Filter valid trajectories (need at least 2 states for intervals)\n",
    "        self.valid_traj_indices = [i for i, t in enumerate(trajectories) if len(t) >= 2]\n",
    "        \n",
    "        # Pre-generate all samples\n",
    "        self.anchors, self.positives, self.negatives = self._generate_all_samples()\n",
    "    \n",
    "    def _normalize_state(self, state):\n",
    "        \"\"\"Normalize state index to [0, 1].\"\"\"\n",
    "        return state / (self.n_states - 1)\n",
    "    \n",
    "    def _generate_all_samples(self):\n",
    "        anchors = []\n",
    "        positives = []\n",
    "        negatives_list = []\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            anchor, positive, negs = self._generate_single_sample()\n",
    "            anchors.append(anchor)\n",
    "            positives.append(positive)\n",
    "            negatives_list.append(negs)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(anchors, dtype=torch.float32),\n",
    "            torch.tensor(positives, dtype=torch.float32),\n",
    "            torch.tensor(negatives_list, dtype=torch.float32),\n",
    "        )\n",
    "    \n",
    "    def _generate_single_sample(self):\n",
    "        \"\"\"Generate a single (anchor, positive, negatives) tuple.\"\"\"\n",
    "        # Sample trajectory\n",
    "        traj_idx = np.random.choice(self.valid_traj_indices)\n",
    "        traj = self.trajectories[traj_idx]\n",
    "        T = len(traj) - 1\n",
    "        \n",
    "        # Sample anchor interval [i, j] where i <= j\n",
    "        j = np.random.randint(0, T + 1)\n",
    "        i = np.random.randint(0, j + 1)\n",
    "        \n",
    "        anchor = [self._normalize_state(traj[i]), self._normalize_state(traj[j])]\n",
    "        \n",
    "        # Sample positive (subinterval): i <= k <= l <= j\n",
    "        l = np.random.randint(i, j + 1)\n",
    "        k = np.random.randint(i, l + 1)\n",
    "        \n",
    "        positive = [self._normalize_state(traj[k]), self._normalize_state(traj[l])]\n",
    "        \n",
    "        # Sample negatives (non-subintervals)\n",
    "        negatives = []\n",
    "        for _ in range(self.n_negatives):\n",
    "            neg = self._sample_negative(traj, i, j, T)\n",
    "            negatives.append(neg)\n",
    "        \n",
    "        return anchor, positive, negatives\n",
    "    \n",
    "    def _sample_negative(self, traj, anchor_i, anchor_j, T, max_attempts=1000):\n",
    "        \"\"\"Sample an interval that is NOT a subinterval of anchor.\"\"\"\n",
    "        for _ in range(max_attempts):\n",
    "            l = np.random.randint(0, T + 1)\n",
    "            k = np.random.randint(0, l + 1)\n",
    "            \n",
    "            # Check if NOT a subinterval (temporal containment)\n",
    "            is_subinterval = (anchor_i <= k) and (l <= anchor_j)\n",
    "            \n",
    "            if not is_subinterval:\n",
    "                return [self._normalize_state(traj[k]), self._normalize_state(traj[l])]\n",
    "        \n",
    "        # Fallback\n",
    "        if anchor_i > 0:\n",
    "            return [self._normalize_state(traj[0]), self._normalize_state(traj[0])]\n",
    "        else:\n",
    "            return [self._normalize_state(traj[T]), self._normalize_state(traj[T])]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.anchors[idx], self.positives[idx], self.negatives[idx]\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = MDPContrastiveDataset(\n",
    "    trajectories=trajectories,\n",
    "    n_states=6,\n",
    "    num_samples=10000,\n",
    "    n_negatives=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Check shapes\n",
    "anchor, positive, negatives = dataset[0]\n",
    "print(f\"Anchor shape: {anchor.shape}\")\n",
    "print(f\"Positive shape: {positive.shape}\")\n",
    "print(f\"Negatives shape: {negatives.shape}\")\n",
    "print(f\"\\nSample anchor: {anchor}\")\n",
    "print(f\"Sample positive: {positive}\")\n",
    "print(f\"Sample negatives[0]: {negatives[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manifold_map(x, manifold):\n",
    "    \"\"\"Map Euclidean tensor to hyperbolic manifold via exponential map.\"\"\"\n",
    "    tangents = TangentTensor(x, man_dim=-1, manifold=manifold)\n",
    "    return manifold.expmap(tangents)\n",
    "\n",
    "\n",
    "class HyperbolicIntervalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode (state, goal) pairs to Poincare ball.\n",
    "    Architecture: 2 Euclidean layers + 2 Hyperbolic layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=2, c=1.0, euc_width=128, hyp_width=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create manifold\n",
    "        curvature = Curvature(value=c, requires_grad=False)\n",
    "        self.manifold = PoincareBall(c=curvature)\n",
    "        \n",
    "        # Euclidean layers\n",
    "        self.euc_layer1 = nn.Linear(2, euc_width)\n",
    "        self.euc_layer2 = nn.Linear(euc_width, hyp_width)\n",
    "        self.euc_relu = nn.ReLU()\n",
    "        \n",
    "        # Hyperbolic layers\n",
    "        self.hyp_layer1 = hnn.HLinear(\n",
    "            in_features=hyp_width,\n",
    "            out_features=hyp_width,\n",
    "            manifold=self.manifold\n",
    "        )\n",
    "        self.hyp_layer2 = hnn.HLinear(\n",
    "            in_features=hyp_width,\n",
    "            out_features=embedding_dim,\n",
    "            manifold=self.manifold\n",
    "        )\n",
    "        self.hyp_relu = hnn.HReLU(manifold=self.manifold)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Euclidean part\n",
    "        x = self.euc_relu(self.euc_layer1(x))\n",
    "        x = self.euc_layer2(x)\n",
    "        \n",
    "        # Map to hyperbolic space\n",
    "        x = manifold_map(x, self.manifold)\n",
    "        \n",
    "        # Hyperbolic part\n",
    "        x = self.hyp_relu(self.hyp_layer1(x))\n",
    "        x = self.hyp_layer2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test encoder\n",
    "model = HyperbolicIntervalEncoder(embedding_dim=2, c=1.0).to(device)\n",
    "test_input = torch.tensor([[0.0, 1.0], [0.2, 0.8]], dtype=torch.float32).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "print(f\"Test output type: {type(test_output)}\")\n",
    "if isinstance(test_output, ManifoldTensor):\n",
    "    print(f\"Test output tensor shape: {test_output.tensor.shape}\")\n",
    "    print(f\"Test output: {test_output.tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(anchor, positive, negatives, manifold, temperature=0.5):\n",
    "    \"\"\"InfoNCE loss using hyperbolic distance.\"\"\"\n",
    "    batch_size = anchor.shape[0] if not isinstance(anchor, ManifoldTensor) else anchor.tensor.shape[0]\n",
    "    num_neg = negatives.shape[1] if not isinstance(negatives, ManifoldTensor) else negatives.tensor.shape[1]\n",
    "    \n",
    "    # Compute positive distance\n",
    "    pos_dist = manifold.dist(x=anchor, y=positive)\n",
    "    \n",
    "    # Expand anchor for negative comparisons\n",
    "    if isinstance(anchor, ManifoldTensor):\n",
    "        anchor_tensor = anchor.tensor.unsqueeze(1).expand(-1, num_neg, -1)\n",
    "        anchor_expanded = ManifoldTensor(anchor_tensor, manifold=manifold)\n",
    "    else:\n",
    "        anchor_expanded = anchor.unsqueeze(1).expand(-1, num_neg, -1)\n",
    "    \n",
    "    neg_dist = manifold.dist(x=anchor_expanded, y=negatives)\n",
    "    \n",
    "    # Extract tensors\n",
    "    if isinstance(pos_dist, ManifoldTensor):\n",
    "        pos_dist = pos_dist.tensor\n",
    "    if isinstance(neg_dist, ManifoldTensor):\n",
    "        neg_dist = neg_dist.tensor\n",
    "    \n",
    "    # Reshape\n",
    "    if pos_dist.dim() > 1:\n",
    "        pos_dist = pos_dist.squeeze(-1)\n",
    "    if neg_dist.dim() > 2:\n",
    "        neg_dist = neg_dist.squeeze(-1)\n",
    "    \n",
    "    # Check for NaN\n",
    "    if torch.isnan(pos_dist).any() or torch.isnan(neg_dist).any():\n",
    "        print(\"WARNING: NaN in distances\")\n",
    "        return torch.tensor(0.0, device=pos_dist.device, requires_grad=True)\n",
    "    \n",
    "    # Convert distances to similarities\n",
    "    pos_sim = -pos_dist / temperature\n",
    "    neg_sim = -neg_dist / temperature\n",
    "    \n",
    "    # Combine for cross-entropy\n",
    "    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)\n",
    "    labels = torch.zeros(batch_size, dtype=torch.long, device=logits.device)\n",
    "    \n",
    "    loss = nn.functional.cross_entropy(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, num_epochs=200, batch_size=32, lr=0.001, temperature=0.1):\n",
    "    \"\"\"Train the hyperbolic encoder.\"\"\"\n",
    "    from hypll.optim import RiemannianAdam\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-5)\n",
    "    \n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for anchor, positive, negatives in dataloader:\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negatives = negatives.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            anchor_emb = model(anchor)\n",
    "            positive_emb = model(positive)\n",
    "            \n",
    "            bs, num_neg, _ = negatives.shape\n",
    "            negatives_emb = model(negatives.view(-1, 2))\n",
    "            \n",
    "            # Reshape negatives\n",
    "            if isinstance(negatives_emb, ManifoldTensor):\n",
    "                neg_tensor = negatives_emb.tensor.view(bs, num_neg, -1)\n",
    "                negatives_emb = ManifoldTensor(neg_tensor, manifold=model.manifold)\n",
    "            else:\n",
    "                negatives_emb = negatives_emb.view(bs, num_neg, -1)\n",
    "            \n",
    "            # Loss\n",
    "            loss = info_nce_loss(anchor_emb, positive_emb, negatives_emb,\n",
    "                                model.manifold, temperature=temperature)\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Skipping batch due to NaN/Inf loss\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        if num_batches > 0:\n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            losses.append(avg_loss)\n",
    "            scheduler.step()\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = HyperbolicIntervalEncoder(embedding_dim=2, c=1.0, euc_width=128, hyp_width=128).to(device)\n",
    "losses = train_model(model, dataset, num_epochs=200, batch_size=32, lr=0.001, temperature=0.1)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hitting Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hitting_times(mdp, start_state, goal_state=5, n_simulations=10000, max_steps=10000):\n",
    "    \"\"\"\n",
    "    Monte Carlo estimation of hitting times from start_state to goal_state\n",
    "    under uniform random policy.\n",
    "    \n",
    "    Returns (mean, variance, std) of hitting times.\n",
    "    \"\"\"\n",
    "    hitting_times = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        state = start_state\n",
    "        steps = 0\n",
    "        \n",
    "        while state != goal_state and steps < max_steps:\n",
    "            state = mdp.step(state, action=None)\n",
    "            steps += 1\n",
    "        \n",
    "        if state == goal_state:\n",
    "            hitting_times.append(steps)\n",
    "    \n",
    "    if len(hitting_times) == 0:\n",
    "        return float('inf'), float('inf'), float('inf')\n",
    "    \n",
    "    return np.mean(hitting_times), np.var(hitting_times), np.std(hitting_times)\n",
    "\n",
    "\n",
    "def compute_all_hitting_times(mdp, n_simulations=10000):\n",
    "    \"\"\"\n",
    "    Compute hitting time statistics for all (start, goal) pairs.\n",
    "    \n",
    "    Returns dict: {start_state: {'mean': ..., 'var': ..., 'std': ...}}\n",
    "    \"\"\"\n",
    "    hitting_stats = {}\n",
    "    goal = mdp.goal_state\n",
    "    \n",
    "    for start in range(mdp.n_states):\n",
    "        if start != goal:\n",
    "            mean, var, std = compute_hitting_times(mdp, start, goal, n_simulations)\n",
    "            hitting_stats[start] = {'mean': mean, 'var': var, 'std': std}\n",
    "            print(f\"{mdp.state_name(start)} -> {mdp.state_name(goal)}: mean={mean:.2f}, var={var:.2f}, std={std:.2f}\")\n",
    "    \n",
    "    return hitting_stats\n",
    "\n",
    "\n",
    "# Compute hitting times\n",
    "print(\"Computing hitting time statistics...\")\n",
    "print(\"=\"*60)\n",
    "hitting_stats = compute_all_hitting_times(mdp, n_simulations=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization on Poincare Ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Utility Functions\n",
    "# =============================================================================\n",
    "\n",
    "def pair_label(start, goal):\n",
    "    \"\"\"\n",
    "    Create label for (start, goal) pair.\n",
    "    - Atomic pairs (s, s) displayed as just the number: \"3\"\n",
    "    - Non-atomic pairs displayed as: \"(1,6)\"\n",
    "    \"\"\"\n",
    "    # Convert from 0-indexed to 1-indexed for display\n",
    "    s, g = start + 1, goal + 1\n",
    "    if start == goal:\n",
    "        return f\"{s}\"\n",
    "    return f\"({s},{g})\"\n",
    "\n",
    "\n",
    "def plot_poincare_disk(ax, title=\"\"):\n",
    "    \"\"\"Draw Poincare disk boundary and origin.\"\"\"\n",
    "    circle = plt.Circle((0, 0), 1, color='black', fill=False, linewidth=2, linestyle='--')\n",
    "    ax.add_patch(circle)\n",
    "    ax.scatter([0], [0], s=60, c='black', marker='+', linewidth=2, zorder=10)\n",
    "    ax.set_xlim(-1.15, 1.15)\n",
    "    ax.set_ylim(-1.15, 1.15)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=13)\n",
    "\n",
    "\n",
    "def hyperbolic_norm(z, curvature=1.0):\n",
    "    \"\"\"Compute hyperbolic distance from origin to point z.\"\"\"\n",
    "    K = curvature\n",
    "    z_norm = np.linalg.norm(z)\n",
    "    z_norm = np.clip(z_norm, 0, (1 / np.sqrt(K)) - 1e-10)\n",
    "    return (2 / np.sqrt(K)) * np.arctanh(np.sqrt(K) * z_norm)\n",
    "\n",
    "\n",
    "def get_embedding(model, start, goal, n_states=6):\n",
    "    \"\"\"\n",
    "    Get the embedding for a specific (start, goal) pair.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained HyperbolicIntervalEncoder\n",
    "        start: Start state (0-indexed)\n",
    "        goal: Goal state (0-indexed)\n",
    "        n_states: Total number of states\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (2,) - the embedding coordinates\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        s_norm = start / (n_states - 1)\n",
    "        g_norm = goal / (n_states - 1)\n",
    "        x = torch.tensor([[s_norm, g_norm]], dtype=torch.float32).to(device)\n",
    "        emb = model(x)\n",
    "        if isinstance(emb, ManifoldTensor):\n",
    "            emb = emb.tensor\n",
    "        return emb.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "def get_all_embeddings(model, n_states=6, include_atomic=True):\n",
    "    \"\"\"\n",
    "    Get embeddings for all (state, goal) pairs.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained HyperbolicIntervalEncoder\n",
    "        n_states: Total number of states\n",
    "        include_atomic: Whether to include atomic (s, s) pairs\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping (start, goal) -> embedding array\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(n_states):\n",
    "            for goal in range(n_states):\n",
    "                if include_atomic or start != goal:\n",
    "                    emb = get_embedding(model, start, goal, n_states)\n",
    "                    embeddings[(start, goal)] = emb\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_state_embeddings(model, mdp):\n",
    "    \"\"\"\n",
    "    Get embeddings for all (state, final_goal) pairs.\n",
    "    This is for backwards compatibility.\n",
    "    \n",
    "    Returns dict: {start_state: embedding_array}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = {}\n",
    "    goal = mdp.goal_state\n",
    "    n_states = mdp.n_states\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for state in range(n_states):\n",
    "            if state != goal:\n",
    "                emb = get_embedding(model, state, goal, n_states)\n",
    "                embeddings[state] = emb\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Get embeddings for all (state, goal=6) pairs\n",
    "embeddings = get_state_embeddings(model, mdp)\n",
    "\n",
    "# Print embeddings\n",
    "print(\"State embeddings (state -> goal=6):\")\n",
    "for state, emb in embeddings.items():\n",
    "    norm = hyperbolic_norm(emb)\n",
    "    angle = np.degrees(np.arctan2(emb[1], emb[0]))\n",
    "    label = pair_label(state, mdp.goal_state)\n",
    "    print(f\"  {label}: ({emb[0]:.4f}, {emb[1]:.4f}), norm={norm:.4f}, angle={angle:.1f}deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mdp_embeddings(embeddings, hitting_stats, mdp):\n",
    "    \"\"\"\n",
    "    Visualize embeddings on the Poincare disk, colored by hitting time statistics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    states = sorted(embeddings.keys())\n",
    "    coords = np.array([embeddings[s] for s in states])\n",
    "    means = np.array([hitting_stats[s]['mean'] for s in states])\n",
    "    vars_ = np.array([hitting_stats[s]['var'] for s in states])\n",
    "    \n",
    "    # Plot 1: Colored by mean hitting time\n",
    "    ax = axes[0]\n",
    "    plot_poincare_disk(ax, \"Colored by Mean Hitting Time\")\n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=means, cmap='viridis',\n",
    "                        s=120, edgecolors='black', linewidth=1.5, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Mean Hitting Time')\n",
    "    \n",
    "    for s in states:\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=12, fontweight='bold',\n",
    "                   xytext=(6, 6), textcoords='offset points')\n",
    "    \n",
    "    # Plot 2: Colored by variance\n",
    "    ax = axes[1]\n",
    "    plot_poincare_disk(ax, \"Colored by Variance of Hitting Time\")\n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=vars_, cmap='plasma',\n",
    "                        s=120, edgecolors='black', linewidth=1.5, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Variance')\n",
    "    \n",
    "    for s in states:\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=12, fontweight='bold',\n",
    "                   xytext=(6, 6), textcoords='offset points')\n",
    "    \n",
    "    # Plot 3: Labeled with all info\n",
    "    ax = axes[2]\n",
    "    plot_poincare_disk(ax, \"All State Embeddings\")\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(states)))\n",
    "    for i, s in enumerate(states):\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.scatter([emb[0]], [emb[1]], c=[colors[i]], s=120,\n",
    "                  edgecolors='black', linewidth=1.5, zorder=5,\n",
    "                  label=f\"{label}: T={hitting_stats[s]['mean']:.1f}\")\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=12, fontweight='bold',\n",
    "                   xytext=(6, 6), textcoords='offset points')\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mdp_embeddings.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_mdp_embeddings(embeddings, hitting_stats, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate cell removed - using cell-20 instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conjecture(embeddings, hitting_stats, mdp):\n",
    "    \"\"\"\n",
    "    Test the conjecture:\n",
    "    - Norm (distance from origin) correlates NEGATIVELY with variance in hitting times\n",
    "      (low variance -> high norm, high variance -> low norm)\n",
    "    - Angular coordinate correlates with mean hitting times\n",
    "    \"\"\"\n",
    "    states = sorted(embeddings.keys())\n",
    "    \n",
    "    # Extract data\n",
    "    coords = np.array([embeddings[s] for s in states])\n",
    "    norms = np.array([hyperbolic_norm(embeddings[s]) for s in states])\n",
    "    angles = np.array([np.arctan2(embeddings[s][1], embeddings[s][0]) for s in states])\n",
    "    \n",
    "    means = np.array([hitting_stats[s]['mean'] for s in states])\n",
    "    vars_ = np.array([hitting_stats[s]['var'] for s in states])\n",
    "    stds = np.array([hitting_stats[s]['std'] for s in states])\n",
    "    \n",
    "    # Compute correlations\n",
    "    corr_norm_var, p_norm_var = stats.spearmanr(norms, vars_)\n",
    "    corr_norm_std, p_norm_std = stats.spearmanr(norms, stds)\n",
    "    corr_angle_mean, p_angle_mean = stats.spearmanr(angles, means)\n",
    "    \n",
    "    # Also compute Pearson\n",
    "    pearson_norm_var = np.corrcoef(norms, vars_)[0, 1]\n",
    "    pearson_angle_mean = np.corrcoef(angles, means)[0, 1]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"CONJECTURE TEST RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n1. NORM vs VARIANCE Conjecture:\")\n",
    "    print(f\"   Spearman correlation: {corr_norm_var:.4f} (p={p_norm_var:.4f})\")\n",
    "    print(f\"   Pearson correlation:  {pearson_norm_var:.4f}\")\n",
    "    print(f\"   Expected: NEGATIVE (low variance -> high norm)\")\n",
    "    norm_var_supported = corr_norm_var < -0.3\n",
    "    print(f\"   Result: {'SUPPORTED' if norm_var_supported else 'NOT SUPPORTED'}\")\n",
    "    \n",
    "    print(\"\\n2. ANGLE vs MEAN Conjecture:\")\n",
    "    print(f\"   Spearman correlation: {corr_angle_mean:.4f} (p={p_angle_mean:.4f})\")\n",
    "    print(f\"   Pearson correlation:  {pearson_angle_mean:.4f}\")\n",
    "    print(f\"   Expected: Strong correlation (similar means -> similar angles)\")\n",
    "    angle_mean_supported = abs(corr_angle_mean) > 0.3\n",
    "    print(f\"   Result: {'SUPPORTED' if angle_mean_supported else 'NOT SUPPORTED'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATE-BY-STATE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Pair':<10} {'Norm':<10} {'Angle(deg)':<12} {'Mean T':<10} {'Var T':<10} {'Std T':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        print(f\"{label:<10} {norms[i]:<10.4f} {np.degrees(angles[i]):<12.1f} \"\n",
    "              f\"{means[i]:<10.2f} {vars_[i]:<10.2f} {stds[i]:<10.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'corr_norm_var': corr_norm_var,\n",
    "        'corr_angle_mean': corr_angle_mean,\n",
    "        'p_norm_var': p_norm_var,\n",
    "        'p_angle_mean': p_angle_mean,\n",
    "        'norms': norms,\n",
    "        'angles': angles,\n",
    "        'means': means,\n",
    "        'vars': vars_,\n",
    "        'states': states,\n",
    "        'norm_var_supported': norm_var_supported,\n",
    "        'angle_mean_supported': angle_mean_supported\n",
    "    }\n",
    "\n",
    "\n",
    "results = test_conjecture(embeddings, hitting_stats, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(embeddings, results, hitting_stats, mdp):\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary plot with Poincare disk, scatter plots, and statistics.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 14))\n",
    "    \n",
    "    # Create grid spec for layout\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    states = results['states']\n",
    "    norms = results['norms']\n",
    "    angles = results['angles']\n",
    "    means = results['means']\n",
    "    vars_ = results['vars']\n",
    "    coords = np.array([embeddings[s] for s in states])\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Plot 1: Poincare Ball (top-left)\n",
    "    # =========================================================================\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    # Draw disk\n",
    "    circle = plt.Circle((0, 0), 1, color='black', fill=False, linewidth=2, linestyle='--')\n",
    "    ax1.add_patch(circle)\n",
    "    ax1.scatter([0], [0], s=60, c='black', marker='+', linewidth=2, zorder=10)\n",
    "    \n",
    "    # Color by variance (inverse - low var = brighter)\n",
    "    scatter = ax1.scatter(coords[:, 0], coords[:, 1], c=vars_, cmap='plasma_r',\n",
    "                         s=100, edgecolors='black', linewidth=1.5, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax1, label='Variance', shrink=0.8)\n",
    "    \n",
    "    for s in states:\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax1.annotate(label, (emb[0], emb[1]), fontsize=11, fontweight='bold',\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax1.set_xlim(-1.15, 1.15)\n",
    "    ax1.set_ylim(-1.15, 1.15)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.grid(True, alpha=0.2)\n",
    "    ax1.set_title('Poincaré Ball Embeddings\\n(colored by variance)', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Plot 2: Norm vs Variance (top-right)\n",
    "    # =========================================================================\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    scatter = ax2.scatter(vars_, norms, c=means, cmap='viridis', s=100, \n",
    "                         edgecolors='black', linewidth=1.5)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Mean Hitting Time', shrink=0.8)\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax2.annotate(label, (vars_[i], norms[i]), fontsize=11, fontweight='bold',\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(vars_, norms, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_trend = np.linspace(min(vars_) - 5, max(vars_) + 5, 100)\n",
    "    ax2.plot(x_trend, p(x_trend), 'r--', linewidth=2, alpha=0.7, label=f'Trend (slope={z[0]:.4f})')\n",
    "    \n",
    "    ax2.set_xlabel('Variance of Hitting Time', fontsize=11)\n",
    "    ax2.set_ylabel('Hyperbolic Norm', fontsize=11)\n",
    "    \n",
    "    # Add correlation annotation\n",
    "    corr = results['corr_norm_var']\n",
    "    status = \"SUPPORTED\" if corr < -0.3 else \"NOT SUPPORTED\"\n",
    "    color = 'green' if corr < -0.3 else 'red'\n",
    "    ax2.set_title(f'Conjecture 1: Norm vs Variance\\nρ = {corr:.3f} ({status})', \n",
    "                 fontsize=13, fontweight='bold', color='black')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Plot 3: Angle vs Mean (bottom-left)\n",
    "    # =========================================================================\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    scatter = ax3.scatter(means, np.degrees(angles), c=vars_, cmap='plasma', s=100,\n",
    "                         edgecolors='black', linewidth=1.5)\n",
    "    plt.colorbar(scatter, ax=ax3, label='Variance', shrink=0.8)\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax3.annotate(label, (means[i], np.degrees(angles[i])), fontsize=11, fontweight='bold',\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(means, np.degrees(angles), 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_trend = np.linspace(min(means) - 1, max(means) + 1, 100)\n",
    "    ax3.plot(x_trend, p(x_trend), 'r--', linewidth=2, alpha=0.7, label=f'Trend (slope={z[0]:.2f})')\n",
    "    \n",
    "    ax3.set_xlabel('Mean Hitting Time', fontsize=11)\n",
    "    ax3.set_ylabel('Angular Coordinate (degrees)', fontsize=11)\n",
    "    \n",
    "    corr = results['corr_angle_mean']\n",
    "    status = \"SUPPORTED\" if abs(corr) > 0.3 else \"NOT SUPPORTED\"\n",
    "    ax3.set_title(f'Conjecture 2: Angle vs Mean\\nρ = {corr:.3f} ({status})', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax3.legend(loc='best')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Plot 4: Summary Statistics (bottom-right)\n",
    "    # =========================================================================\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    norm_var_status = \"SUPPORTED\" if results['norm_var_supported'] else \"NOT SUPPORTED\"\n",
    "    angle_mean_status = \"SUPPORTED\" if results['angle_mean_supported'] else \"NOT SUPPORTED\"\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════╗\n",
    "║              HYPOTHESIS TEST SUMMARY                      ║\n",
    "╠══════════════════════════════════════════════════════════╣\n",
    "║                                                          ║\n",
    "║  CONJECTURE 1: Norm ∝ 1/Variance                         ║\n",
    "║  ─────────────────────────────────────────────────────   ║\n",
    "║  Expected: Low variance → High norm (negative corr)      ║\n",
    "║  Spearman ρ = {results['corr_norm_var']:+.4f}  (p = {results['p_norm_var']:.4f})                  ║\n",
    "║  Status: {norm_var_status:<20}                       ║\n",
    "║                                                          ║\n",
    "╠══════════════════════════════════════════════════════════╣\n",
    "║                                                          ║\n",
    "║  CONJECTURE 2: Angle ∝ Mean                              ║\n",
    "║  ─────────────────────────────────────────────────────   ║\n",
    "║  Expected: Similar mean → Similar angle (strong corr)    ║\n",
    "║  Spearman ρ = {results['corr_angle_mean']:+.4f}  (p = {results['p_angle_mean']:.4f})                  ║\n",
    "║  Status: {angle_mean_status:<20}                       ║\n",
    "║                                                          ║\n",
    "╠══════════════════════════════════════════════════════════╣\n",
    "║                                                          ║\n",
    "║  STATE STATISTICS:                                       ║\n",
    "║  ─────────────────────────────────────────────────────   ║\"\"\"\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        summary_text += f\"\\n║  {label}: norm={norms[i]:.2f}, angle={np.degrees(angles[i]):+6.1f}°, \"\n",
    "        summary_text += f\"μ={means[i]:5.1f}, σ²={vars_[i]:6.1f}  ║\"\n",
    "    \n",
    "    summary_text += \"\"\"\n",
    "║                                                          ║\n",
    "╚══════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "    \n",
    "    ax4.text(0.02, 0.98, summary_text, transform=ax4.transAxes, fontsize=10,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.9, edgecolor='black'))\n",
    "    \n",
    "    plt.suptitle('MDP Hyperbolic Embedding Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mdp_summary.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_summary(embeddings, results, hitting_stats, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conjecture_analysis(results, mdp):\n",
    "    \"\"\"\n",
    "    Detailed visualization of the conjecture analysis.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    norms = results['norms']\n",
    "    angles = results['angles']\n",
    "    means = results['means']\n",
    "    vars_ = results['vars']\n",
    "    states = results['states']\n",
    "    \n",
    "    # Plot 1: Norm vs Variance\n",
    "    ax = axes[0, 0]\n",
    "    scatter = ax.scatter(vars_, norms, c=means, cmap='viridis', s=200, edgecolors='black', linewidth=2)\n",
    "    plt.colorbar(scatter, ax=ax, label='Mean Hitting Time')\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (vars_[i], norms[i]), fontsize=12, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(vars_, norms, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_trend = np.linspace(min(vars_), max(vars_), 100)\n",
    "    ax.plot(x_trend, p(x_trend), 'r--', linewidth=2, label=f'Trend (slope={z[0]:.4f})')\n",
    "    \n",
    "    ax.set_xlabel('Variance of Hitting Time', fontsize=12)\n",
    "    ax.set_ylabel('Hyperbolic Norm', fontsize=12)\n",
    "    ax.set_title(f'Conjecture 1: Norm vs Variance\\nCorr={results[\"corr_norm_var\"]:.3f}', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Angle vs Mean\n",
    "    ax = axes[0, 1]\n",
    "    scatter = ax.scatter(means, np.degrees(angles), c=vars_, cmap='plasma', s=200, edgecolors='black', linewidth=2)\n",
    "    plt.colorbar(scatter, ax=ax, label='Variance')\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (means[i], np.degrees(angles[i])), fontsize=12, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax.set_xlabel('Mean Hitting Time', fontsize=12)\n",
    "    ax.set_ylabel('Angular Coordinate (degrees)', fontsize=12)\n",
    "    ax.set_title(f'Conjecture 2: Angle vs Mean\\nCorr={results[\"corr_angle_mean\"]:.3f}', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Norm vs Mean\n",
    "    ax = axes[1, 0]\n",
    "    scatter = ax.scatter(means, norms, c=vars_, cmap='plasma', s=200, edgecolors='black', linewidth=2)\n",
    "    plt.colorbar(scatter, ax=ax, label='Variance')\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (means[i], norms[i]), fontsize=12, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    corr_norm_mean = np.corrcoef(norms, means)[0, 1]\n",
    "    ax.set_xlabel('Mean Hitting Time', fontsize=12)\n",
    "    ax.set_ylabel('Hyperbolic Norm', fontsize=12)\n",
    "    ax.set_title(f'Additional: Norm vs Mean\\nCorr={corr_norm_mean:.3f}', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Summary stats\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    CONJECTURE ANALYSIS SUMMARY\n",
    "    {'='*50}\n",
    "    \n",
    "    Conjecture 1: Norm ~ Variance\n",
    "    -----------------------------\n",
    "    Spearman correlation: {results['corr_norm_var']:.4f}\n",
    "    Interpretation: {'SUPPORTED' if results['corr_norm_var'] > 0.3 else 'WEAK/NOT SUPPORTED'}\n",
    "    \n",
    "    Conjecture 2: Angle ~ Mean\n",
    "    ---------------------------\n",
    "    Spearman correlation: {results['corr_angle_mean']:.4f}\n",
    "    Interpretation: {'SUPPORTED' if abs(results['corr_angle_mean']) > 0.3 else 'WEAK/NOT SUPPORTED'}\n",
    "    \n",
    "    {'='*50}\n",
    "    \n",
    "    Note: The MDP has structure that affects embeddings:\n",
    "    - States 4, 5: 1-step to goal (low var, low mean)\n",
    "    - States 2, 3: Geometric waiting (high var, high mean)\n",
    "    - State 1: Depends on action choice\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('conjecture_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_conjecture_analysis(results, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_state_pairs(model, mdp):\n",
    "    \"\"\"\n",
    "    Plot embeddings for ALL (state, goal) pairs, not just (state, final_goal).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_states = mdp.n_states\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_poincare_disk(ax, \"All (State, Goal) Pair Embeddings\")\n",
    "    \n",
    "    all_embeddings = []\n",
    "    labels = []\n",
    "    colors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(n_states):\n",
    "            for goal in range(n_states):\n",
    "                if start != goal:  # Skip self-loops\n",
    "                    s_norm = start / (n_states - 1)\n",
    "                    g_norm = goal / (n_states - 1)\n",
    "                    \n",
    "                    x = torch.tensor([[s_norm, g_norm]], dtype=torch.float32).to(device)\n",
    "                    emb = model(x)\n",
    "                    \n",
    "                    if isinstance(emb, ManifoldTensor):\n",
    "                        emb = emb.tensor\n",
    "                    \n",
    "                    emb = emb.squeeze(0).cpu().numpy()\n",
    "                    all_embeddings.append(emb)\n",
    "                    labels.append(pair_label(start, goal))\n",
    "                    colors.append(goal)  # Color by goal state\n",
    "    \n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    colors = np.array(colors)\n",
    "    \n",
    "    scatter = ax.scatter(all_embeddings[:, 0], all_embeddings[:, 1], \n",
    "                        c=colors, cmap='tab10', s=80, edgecolors='black', linewidth=1, zorder=5)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, label in enumerate(labels):\n",
    "        ax.annotate(label, all_embeddings[i], fontsize=7, \n",
    "                   xytext=(3, 3), textcoords='offset points')\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax, label='Goal State Index')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_state_pairs.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_all_state_pairs(model, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polar_representation(embeddings, hitting_stats, mdp):\n",
    "    \"\"\"\n",
    "    Polar plot showing angle vs hyperbolic norm.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': 'polar'})\n",
    "    \n",
    "    states = sorted(embeddings.keys())\n",
    "    \n",
    "    angles = [np.arctan2(embeddings[s][1], embeddings[s][0]) for s in states]\n",
    "    norms = [hyperbolic_norm(embeddings[s]) for s in states]\n",
    "    means = [hitting_stats[s]['mean'] for s in states]\n",
    "    vars_ = [hitting_stats[s]['var'] for s in states]\n",
    "    \n",
    "    # Size by inverse variance (low variance = larger), color by mean\n",
    "    max_var = max(vars_) if max(vars_) > 0 else 1\n",
    "    sizes = 50 + 100 * (1 - np.array(vars_) / max_var)\n",
    "    \n",
    "    scatter = ax.scatter(angles, norms, c=means, cmap='viridis', s=sizes, \n",
    "                        edgecolors='black', linewidth=1.5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Mean Hitting Time', pad=0.1)\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (angles[i], norms[i]), fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_title('Polar View: Angle vs Hyperbolic Norm\\n(Size ~ 1/Variance, Color ~ Mean)', fontsize=13, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('polar_embeddings.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_polar_representation(embeddings, hitting_stats, mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### MDP Structure Analysis\n",
    "\n",
    "The MDP has distinct structural properties that affect hitting times:\n",
    "\n",
    "1. **States 4 and 5 (S4, S5)**: Deterministic 1-step transitions to goal\n",
    "   - Mean hitting time: 1\n",
    "   - Variance: 0\n",
    "   \n",
    "2. **States 2 and 3 (S2, S3)**: Geometric waiting times due to self-loops\n",
    "   - Mean hitting time: ~10-11 (expected value of geometric(0.1) + 1)\n",
    "   - Variance: ~90 (variance of geometric distribution)\n",
    "   \n",
    "3. **State 1 (S1)**: Depends on action selection\n",
    "   - Under random policy: mix of 2-step (via a11) and longer paths (via a12, a13)\n",
    "\n",
    "### Conjecture Interpretation\n",
    "\n",
    "The conjecture states:\n",
    "- **Norm ~ 1/Variance**: States with **lower** hitting time variance should be embedded **farther** from the origin (higher norm)\n",
    "- **Angle ~ Mean**: States with similar mean hitting times should have similar angular coordinates\n",
    "\n",
    "This would create a structure where:\n",
    "- S4, S5 (low variance, deterministic) should have **high norms** (far from origin)\n",
    "- S2, S3 (high variance, geometric waiting) should have **low norms** (near origin)\n",
    "- States with similar means lie along similar radial directions\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In hyperbolic space, the origin represents maximal uncertainty/abstraction. States with:\n",
    "- **Low variance** (predictable hitting times) are more \"specific\" → farther from origin\n",
    "- **High variance** (unpredictable hitting times) are more \"abstract\" → closer to origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining completed with {len(trajectories)} trajectories\")\n",
    "print(f\"Final training loss: {losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Conjecture 1: Norm ~ 1/Variance ---\")\n",
    "print(f\"Spearman correlation: {results['corr_norm_var']:.4f}\")\n",
    "print(f\"Expected: Negative (low variance -> high norm)\")\n",
    "print(f\"Status: {'SUPPORTED' if results['norm_var_supported'] else 'NOT SUPPORTED'}\")\n",
    "\n",
    "print(f\"\\n--- Conjecture 2: Angle ~ Mean ---\")\n",
    "print(f\"Spearman correlation: {results['corr_angle_mean']:.4f}\")\n",
    "print(f\"Expected: Strong correlation\")\n",
    "print(f\"Status: {'SUPPORTED' if results['angle_mean_supported'] else 'NOT SUPPORTED'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"See the summary plot above for detailed visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining completed with {len(trajectories)} trajectories\")\n",
    "print(f\"Final training loss: {losses[-1]:.4f}\")\n",
    "print(f\"\\nConjecture 1 (Norm ~ Variance): correlation = {results['corr_norm_var']:.4f}\")\n",
    "print(f\"Conjecture 2 (Angle ~ Mean): correlation = {results['corr_angle_mean']:.4f}\")\n",
    "print(\"\\nSee visualizations above for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal-Conditioned Behavioral Cloning (GCBC)\n",
    "\n",
    "We train two policies:\n",
    "1. **Raw State Policy**: Takes (state, goal) as one-hot encoded inputs\n",
    "2. **Embedding Policy**: Takes hyperbolic embeddings of (state, goal) pairs\n",
    "\n",
    "Both are trained via behavioral cloning on the generated trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GCBC Dataset Creation\n",
    "# =============================================================================\n",
    "\n",
    "def create_gcbc_dataset(trajectories, mdp, goal_type=\"final\"):\n",
    "    \"\"\"\n",
    "    Create dataset for goal-conditioned behavioral cloning.\n",
    "    \n",
    "    For each (state, action) in a trajectory, we create a sample:\n",
    "    - state: current state (0-indexed)\n",
    "    - goal: final state of trajectory (or random future state)\n",
    "    - action: action taken (inferred from transition)\n",
    "    \n",
    "    Args:\n",
    "        trajectories: List of state sequences\n",
    "        mdp: SimpleMDP instance\n",
    "        goal_type: \"final\" (trajectory endpoint) or \"random\" (random future state)\n",
    "    \n",
    "    Returns:\n",
    "        states, goals, actions as numpy arrays\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    goals = []\n",
    "    actions = []\n",
    "    \n",
    "    for traj in trajectories:\n",
    "        final_state = traj[-1]  # Usually the goal state\n",
    "        \n",
    "        for t in range(len(traj) - 1):\n",
    "            state = traj[t]\n",
    "            next_state = traj[t + 1]\n",
    "            \n",
    "            if state == mdp.goal_state:\n",
    "                continue  # Skip terminal state\n",
    "            \n",
    "            # Infer action from (state, next_state) transition\n",
    "            action = infer_action(mdp, state, next_state)\n",
    "            if action is None:\n",
    "                continue  # Skip invalid transitions\n",
    "            \n",
    "            if goal_type == \"final\":\n",
    "                goal = final_state\n",
    "            else:  # random future\n",
    "                future_idx = np.random.randint(t + 1, len(traj))\n",
    "                goal = traj[future_idx]\n",
    "            \n",
    "            states.append(state)\n",
    "            goals.append(goal)\n",
    "            actions.append(action)\n",
    "    \n",
    "    return np.array(states), np.array(goals), np.array(actions)\n",
    "\n",
    "\n",
    "def infer_action(mdp, state, next_state):\n",
    "    \"\"\"\n",
    "    Infer which action was taken to transition from state to next_state.\n",
    "    Returns action index or None if transition is impossible.\n",
    "    \"\"\"\n",
    "    for action in range(mdp.n_actions[state]):\n",
    "        transitions = mdp.get_transitions(state, action)\n",
    "        for ns, prob in transitions:\n",
    "            if ns == next_state and prob > 0:\n",
    "                return action\n",
    "    return None\n",
    "\n",
    "\n",
    "# Create GCBC dataset\n",
    "gcbc_states, gcbc_goals, gcbc_actions = create_gcbc_dataset(trajectories, mdp, goal_type=\"final\")\n",
    "\n",
    "print(f\"GCBC Dataset created:\")\n",
    "print(f\"  Total samples: {len(gcbc_states)}\")\n",
    "print(f\"  Unique states: {np.unique(gcbc_states)}\")\n",
    "print(f\"  Unique goals: {np.unique(gcbc_goals)}\")\n",
    "print(f\"  Unique actions: {np.unique(gcbc_actions)}\")\n",
    "\n",
    "# Distribution of actions per state\n",
    "print(\"\\nAction distribution per state:\")\n",
    "for s in range(mdp.n_states - 1):  # Exclude goal state\n",
    "    mask = gcbc_states == s\n",
    "    if mask.sum() > 0:\n",
    "        action_counts = np.bincount(gcbc_actions[mask], minlength=3)\n",
    "        print(f\"  State {s+1}: {dict(zip(range(len(action_counts)), action_counts))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GCBC Policy Networks\n",
    "# =============================================================================\n",
    "\n",
    "class GCBCPolicyRaw(nn.Module):\n",
    "    \"\"\"\n",
    "    Goal-Conditioned Policy using raw (one-hot) state representations.\n",
    "    \n",
    "    Input: concatenated one-hot encodings of state and goal\n",
    "    Output: action logits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states=6, n_actions=3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Input: one-hot(state) + one-hot(goal) = 2 * n_states\n",
    "        input_dim = 2 * n_states\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, states, goals):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            states: tensor of shape (batch,) with state indices\n",
    "            goals: tensor of shape (batch,) with goal indices\n",
    "        Returns:\n",
    "            action logits of shape (batch, n_actions)\n",
    "        \"\"\"\n",
    "        # One-hot encode\n",
    "        state_onehot = torch.nn.functional.one_hot(states.long(), self.n_states).float()\n",
    "        goal_onehot = torch.nn.functional.one_hot(goals.long(), self.n_states).float()\n",
    "        \n",
    "        # Concatenate\n",
    "        x = torch.cat([state_onehot, goal_onehot], dim=-1)\n",
    "        \n",
    "        return self.network(x)\n",
    "    \n",
    "    def get_action(self, state, goal, deterministic=True):\n",
    "        \"\"\"Get action for a single (state, goal) pair.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor([state], dtype=torch.long, device=device)\n",
    "            g = torch.tensor([goal], dtype=torch.long, device=device)\n",
    "            logits = self.forward(s, g)\n",
    "            \n",
    "            if deterministic:\n",
    "                return logits.argmax(dim=-1).item()\n",
    "            else:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                return torch.multinomial(probs, 1).item()\n",
    "\n",
    "\n",
    "class GCBCPolicyEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Goal-Conditioned Policy using hyperbolic embeddings.\n",
    "    \n",
    "    Input: embedding of (state, goal) pair from trained encoder\n",
    "    Output: action logits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=2, n_actions=3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: tensor of shape (batch, embedding_dim)\n",
    "        Returns:\n",
    "            action logits of shape (batch, n_actions)\n",
    "        \"\"\"\n",
    "        return self.network(embeddings)\n",
    "    \n",
    "    def get_action(self, embedding, deterministic=True):\n",
    "        \"\"\"Get action for a single embedding.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            emb = torch.tensor(embedding, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            logits = self.forward(emb)\n",
    "            \n",
    "            if deterministic:\n",
    "                return logits.argmax(dim=-1).item()\n",
    "            else:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                return torch.multinomial(probs, 1).item()\n",
    "\n",
    "\n",
    "# Test policy networks\n",
    "raw_policy = GCBCPolicyRaw(n_states=6, n_actions=3).to(device)\n",
    "emb_policy = GCBCPolicyEmbedding(embedding_dim=2, n_actions=3).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "test_states = torch.tensor([0, 1, 2], dtype=torch.long, device=device)\n",
    "test_goals = torch.tensor([5, 5, 5], dtype=torch.long, device=device)\n",
    "test_emb = torch.randn(3, 2, device=device)\n",
    "\n",
    "print(\"Raw Policy output shape:\", raw_policy(test_states, test_goals).shape)\n",
    "print(\"Embedding Policy output shape:\", emb_policy(test_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Functions\n",
    "# =============================================================================\n",
    "\n",
    "def train_raw_policy(policy, states, goals, actions, epochs=100, batch_size=64, lr=0.001):\n",
    "    \"\"\"Train the raw state policy via behavioral cloning.\"\"\"\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    states_t = torch.tensor(states, dtype=torch.long, device=device)\n",
    "    goals_t = torch.tensor(goals, dtype=torch.long, device=device)\n",
    "    actions_t = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    \n",
    "    n_samples = len(states)\n",
    "    losses = []\n",
    "    \n",
    "    policy.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(n_samples)\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            batch_states = states_t[idx]\n",
    "            batch_goals = goals_t[idx]\n",
    "            batch_actions = actions_t[idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = policy(batch_states, batch_goals)\n",
    "            loss = criterion(logits, batch_actions)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def train_embedding_policy(policy, encoder, states, goals, actions, epochs=100, batch_size=64, lr=0.001):\n",
    "    \"\"\"Train the embedding policy via behavioral cloning.\"\"\"\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Get embeddings for all (state, goal) pairs\n",
    "    encoder.eval()\n",
    "    n_states = 6\n",
    "    \n",
    "    # Pre-compute embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        for s, g in zip(states, goals):\n",
    "            emb = get_embedding(encoder, s, g, n_states)\n",
    "            embeddings.append(emb)\n",
    "        embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    embeddings_t = torch.tensor(embeddings, dtype=torch.float32, device=device)\n",
    "    actions_t = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    \n",
    "    n_samples = len(states)\n",
    "    losses = []\n",
    "    \n",
    "    policy.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(n_samples)\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            batch_emb = embeddings_t[idx]\n",
    "            batch_actions = actions_t[idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = policy(batch_emb)\n",
    "            loss = criterion(logits, batch_actions)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Train both policies\n",
    "print(\"=\"*60)\n",
    "print(\"Training Raw State Policy\")\n",
    "print(\"=\"*60)\n",
    "raw_policy = GCBCPolicyRaw(n_states=6, n_actions=3, hidden_dim=64).to(device)\n",
    "raw_losses = train_raw_policy(raw_policy, gcbc_states, gcbc_goals, gcbc_actions, epochs=100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Embedding Policy\")\n",
    "print(\"=\"*60)\n",
    "emb_policy = GCBCPolicyEmbedding(embedding_dim=2, n_actions=3, hidden_dim=64).to(device)\n",
    "emb_losses = train_embedding_policy(emb_policy, model, gcbc_states, gcbc_goals, gcbc_actions, epochs=100)\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(raw_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Raw State Policy Training')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(emb_losses)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Embedding Policy Training')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Evaluate both policies (raw and embedding-based) in two modes:\n",
    "1. **Deterministic**: Always take argmax action\n",
    "2. **Stochastic**: Sample from action distribution\n",
    "\n",
    "Metrics: Average steps to reach goal from start state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Policy Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_raw_policy(policy, mdp, n_episodes=1000, max_steps=1000, deterministic=True):\n",
    "    \"\"\"\n",
    "    Evaluate raw state policy.\n",
    "    \n",
    "    Args:\n",
    "        policy: GCBCPolicyRaw\n",
    "        mdp: SimpleMDP\n",
    "        n_episodes: Number of evaluation episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "        deterministic: If True, use argmax; else sample from distribution\n",
    "    \n",
    "    Returns:\n",
    "        dict with mean_steps, std_steps, success_rate\n",
    "    \"\"\"\n",
    "    policy.eval()\n",
    "    steps_list = []\n",
    "    successes = 0\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state = mdp.start_state\n",
    "        goal = mdp.goal_state\n",
    "        steps = 0\n",
    "        \n",
    "        while state != goal and steps < max_steps:\n",
    "            # Get action from policy\n",
    "            action = policy.get_action(state, goal, deterministic=deterministic)\n",
    "            \n",
    "            # Clip action to valid range for this state\n",
    "            action = min(action, mdp.n_actions[state] - 1)\n",
    "            \n",
    "            # Take step in environment\n",
    "            state = mdp.step(state, action)\n",
    "            steps += 1\n",
    "        \n",
    "        if state == goal:\n",
    "            successes += 1\n",
    "            steps_list.append(steps)\n",
    "        else:\n",
    "            steps_list.append(max_steps)  # Failure case\n",
    "    \n",
    "    return {\n",
    "        'mean_steps': np.mean(steps_list),\n",
    "        'std_steps': np.std(steps_list),\n",
    "        'success_rate': successes / n_episodes,\n",
    "        'steps_list': steps_list\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_embedding_policy(policy, encoder, mdp, n_episodes=1000, max_steps=1000, deterministic=True):\n",
    "    \"\"\"\n",
    "    Evaluate embedding policy.\n",
    "    \n",
    "    Args:\n",
    "        policy: GCBCPolicyEmbedding\n",
    "        encoder: HyperbolicIntervalEncoder\n",
    "        mdp: SimpleMDP\n",
    "        n_episodes: Number of evaluation episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "        deterministic: If True, use argmax; else sample from distribution\n",
    "    \n",
    "    Returns:\n",
    "        dict with mean_steps, std_steps, success_rate\n",
    "    \"\"\"\n",
    "    policy.eval()\n",
    "    encoder.eval()\n",
    "    steps_list = []\n",
    "    successes = 0\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state = mdp.start_state\n",
    "        goal = mdp.goal_state\n",
    "        steps = 0\n",
    "        \n",
    "        while state != goal and steps < max_steps:\n",
    "            # Get embedding\n",
    "            emb = get_embedding(encoder, state, goal, mdp.n_states)\n",
    "            \n",
    "            # Get action from policy\n",
    "            action = policy.get_action(emb, deterministic=deterministic)\n",
    "            \n",
    "            # Clip action to valid range for this state\n",
    "            action = min(action, mdp.n_actions[state] - 1)\n",
    "            \n",
    "            # Take step in environment\n",
    "            state = mdp.step(state, action)\n",
    "            steps += 1\n",
    "        \n",
    "        if state == goal:\n",
    "            successes += 1\n",
    "            steps_list.append(steps)\n",
    "        else:\n",
    "            steps_list.append(max_steps)\n",
    "    \n",
    "    return {\n",
    "        'mean_steps': np.mean(steps_list),\n",
    "        'std_steps': np.std(steps_list),\n",
    "        'success_rate': successes / n_episodes,\n",
    "        'steps_list': steps_list\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_random_policy(mdp, n_episodes=1000, max_steps=1000):\n",
    "    \"\"\"Evaluate random policy (baseline).\"\"\"\n",
    "    steps_list = []\n",
    "    successes = 0\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state = mdp.start_state\n",
    "        steps = 0\n",
    "        \n",
    "        while state != mdp.goal_state and steps < max_steps:\n",
    "            state = mdp.step(state, action=None)  # Random action\n",
    "            steps += 1\n",
    "        \n",
    "        if state == mdp.goal_state:\n",
    "            successes += 1\n",
    "            steps_list.append(steps)\n",
    "        else:\n",
    "            steps_list.append(max_steps)\n",
    "    \n",
    "    return {\n",
    "        'mean_steps': np.mean(steps_list),\n",
    "        'std_steps': np.std(steps_list),\n",
    "        'success_rate': successes / n_episodes,\n",
    "        'steps_list': steps_list\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate all policies\n",
    "print(\"=\"*70)\n",
    "print(\"POLICY EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_eval_episodes = 5000\n",
    "\n",
    "# Random baseline\n",
    "print(\"\\nRandom Policy (Baseline):\")\n",
    "random_results = evaluate_random_policy(mdp, n_episodes=n_eval_episodes)\n",
    "print(f\"  Mean steps: {random_results['mean_steps']:.2f} ± {random_results['std_steps']:.2f}\")\n",
    "print(f\"  Success rate: {random_results['success_rate']*100:.1f}%\")\n",
    "\n",
    "# Raw policy - deterministic\n",
    "print(\"\\nRaw State Policy (Deterministic):\")\n",
    "raw_det_results = evaluate_raw_policy(raw_policy, mdp, n_episodes=n_eval_episodes, deterministic=True)\n",
    "print(f\"  Mean steps: {raw_det_results['mean_steps']:.2f} ± {raw_det_results['std_steps']:.2f}\")\n",
    "print(f\"  Success rate: {raw_det_results['success_rate']*100:.1f}%\")\n",
    "\n",
    "# Raw policy - stochastic\n",
    "print(\"\\nRaw State Policy (Stochastic):\")\n",
    "raw_stoch_results = evaluate_raw_policy(raw_policy, mdp, n_episodes=n_eval_episodes, deterministic=False)\n",
    "print(f\"  Mean steps: {raw_stoch_results['mean_steps']:.2f} ± {raw_stoch_results['std_steps']:.2f}\")\n",
    "print(f\"  Success rate: {raw_stoch_results['success_rate']*100:.1f}%\")\n",
    "\n",
    "# Embedding policy - deterministic\n",
    "print(\"\\nEmbedding Policy (Deterministic):\")\n",
    "emb_det_results = evaluate_embedding_policy(emb_policy, model, mdp, n_episodes=n_eval_episodes, deterministic=True)\n",
    "print(f\"  Mean steps: {emb_det_results['mean_steps']:.2f} ± {emb_det_results['std_steps']:.2f}\")\n",
    "print(f\"  Success rate: {emb_det_results['success_rate']*100:.1f}%\")\n",
    "\n",
    "# Embedding policy - stochastic\n",
    "print(\"\\nEmbedding Policy (Stochastic):\")\n",
    "emb_stoch_results = evaluate_embedding_policy(emb_policy, model, mdp, n_episodes=n_eval_episodes, deterministic=False)\n",
    "print(f\"  Mean steps: {emb_stoch_results['mean_steps']:.2f} ± {emb_stoch_results['std_steps']:.2f}\")\n",
    "print(f\"  Success rate: {emb_stoch_results['success_rate']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of mean steps\n",
    "ax = axes[0]\n",
    "policies = ['Random', 'Raw\\n(Det)', 'Raw\\n(Stoch)', 'Emb\\n(Det)', 'Emb\\n(Stoch)']\n",
    "means = [random_results['mean_steps'], raw_det_results['mean_steps'], \n",
    "         raw_stoch_results['mean_steps'], emb_det_results['mean_steps'], \n",
    "         emb_stoch_results['mean_steps']]\n",
    "stds = [random_results['std_steps'], raw_det_results['std_steps'], \n",
    "        raw_stoch_results['std_steps'], emb_det_results['std_steps'], \n",
    "        emb_stoch_results['std_steps']]\n",
    "\n",
    "colors = ['gray', 'steelblue', 'lightblue', 'coral', 'lightsalmon']\n",
    "bars = ax.bar(policies, means, yerr=stds, capsize=5, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Mean Steps to Goal', fontsize=12)\n",
    "ax.set_title('Policy Evaluation: Steps to Goal', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            f'{mean:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Success rate bar plot\n",
    "ax = axes[1]\n",
    "success_rates = [random_results['success_rate']*100, raw_det_results['success_rate']*100,\n",
    "                 raw_stoch_results['success_rate']*100, emb_det_results['success_rate']*100,\n",
    "                 emb_stoch_results['success_rate']*100]\n",
    "\n",
    "bars = ax.bar(policies, success_rates, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Success Rate (%)', fontsize=12)\n",
    "ax.set_title('Policy Evaluation: Success Rate', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 105)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f'{rate:.0f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('policy_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Policy':<25} {'Mean Steps':>12} {'Std':>10} {'Success':>12}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Random (Baseline)':<25} {random_results['mean_steps']:>12.2f} {random_results['std_steps']:>10.2f} {random_results['success_rate']*100:>11.1f}%\")\n",
    "print(f\"{'Raw State (Det)':<25} {raw_det_results['mean_steps']:>12.2f} {raw_det_results['std_steps']:>10.2f} {raw_det_results['success_rate']*100:>11.1f}%\")\n",
    "print(f\"{'Raw State (Stoch)':<25} {raw_stoch_results['mean_steps']:>12.2f} {raw_stoch_results['std_steps']:>10.2f} {raw_stoch_results['success_rate']*100:>11.1f}%\")\n",
    "print(f\"{'Embedding (Det)':<25} {emb_det_results['mean_steps']:>12.2f} {emb_det_results['std_steps']:>10.2f} {emb_det_results['success_rate']*100:>11.1f}%\")\n",
    "print(f\"{'Embedding (Stoch)':<25} {emb_stoch_results['mean_steps']:>12.2f} {emb_stoch_results['std_steps']:>10.2f} {emb_stoch_results['success_rate']*100:>11.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Planning Mechanism\n",
    "\n",
    "The planning mechanism works as follows:\n",
    "\n",
    "1. **Compute embedding** φ(s₁, s₂) for the (start, goal) pair\n",
    "2. **Find subgoal**: Find the lowest-norm atomic embedding φ(s', s') that lies along the radial line through φ(s₁, s₂)\n",
    "3. **Decode state**: Map the embedding back to a concrete state s'\n",
    "4. **Recurse**: Apply the algorithm recursively on (s₁, s') and (s', s₂)\n",
    "5. **Display plan**: Show the resulting hierarchical plan\n",
    "\n",
    "The intuition is that atomic states (s, s) with low norm represent \"waypoints\" that are easy to reach (low variance in hitting time), and the radial line represents the \"direction\" of the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Hyperbolic Planning\n",
    "# =============================================================================\n",
    "\n",
    "def get_atomic_embeddings(model, mdp):\n",
    "    \"\"\"\n",
    "    Get embeddings for all atomic (state, state) pairs.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {state: embedding_array}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    atomic_emb = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for s in range(mdp.n_states):\n",
    "            emb = get_embedding(model, s, s, mdp.n_states)\n",
    "            atomic_emb[s] = emb\n",
    "    \n",
    "    return atomic_emb\n",
    "\n",
    "\n",
    "def angular_distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Compute angular distance between two vectors.\n",
    "    Returns angle in radians (0 to π).\n",
    "    \"\"\"\n",
    "    # Normalize vectors\n",
    "    v1_norm = v1 / (np.linalg.norm(v1) + 1e-10)\n",
    "    v2_norm = v2 / (np.linalg.norm(v2) + 1e-10)\n",
    "    \n",
    "    # Dot product gives cosine of angle\n",
    "    cos_angle = np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)\n",
    "    \n",
    "    return np.arccos(cos_angle)\n",
    "\n",
    "\n",
    "def find_subgoal_on_radial_line(target_emb, atomic_embeddings, start_state, goal_state, \n",
    "                                 angle_threshold=np.pi/4, exclude_states=None):\n",
    "    \"\"\"\n",
    "    Find the best subgoal along the radial line from origin through target_emb.\n",
    "    \n",
    "    The subgoal should be:\n",
    "    - An atomic state (s', s') that is different from start and goal\n",
    "    - Close to the radial line (low angular distance)\n",
    "    - Has the lowest hyperbolic norm among valid candidates\n",
    "    \n",
    "    Args:\n",
    "        target_emb: Embedding of (start, goal) pair\n",
    "        atomic_embeddings: Dict of {state: embedding} for atomic states\n",
    "        start_state: Start state (to exclude)\n",
    "        goal_state: Goal state (to exclude)\n",
    "        angle_threshold: Maximum angular deviation from radial line\n",
    "        exclude_states: Additional states to exclude\n",
    "    \n",
    "    Returns:\n",
    "        best_state: The best subgoal state, or None if no valid candidate\n",
    "        best_emb: Embedding of the subgoal\n",
    "    \"\"\"\n",
    "    if exclude_states is None:\n",
    "        exclude_states = set()\n",
    "    \n",
    "    exclude_states = set(exclude_states) | {start_state, goal_state}\n",
    "    \n",
    "    best_state = None\n",
    "    best_norm = float('inf')\n",
    "    best_emb = None\n",
    "    \n",
    "    target_angle = np.arctan2(target_emb[1], target_emb[0])\n",
    "    \n",
    "    for state, emb in atomic_embeddings.items():\n",
    "        if state in exclude_states:\n",
    "            continue\n",
    "        \n",
    "        # Check angular distance from radial line\n",
    "        angle_dist = angular_distance(target_emb, emb)\n",
    "        \n",
    "        if angle_dist <= angle_threshold:\n",
    "            norm = hyperbolic_norm(emb)\n",
    "            if norm < best_norm:\n",
    "                best_norm = norm\n",
    "                best_state = state\n",
    "                best_emb = emb\n",
    "    \n",
    "    return best_state, best_emb\n",
    "\n",
    "\n",
    "def hyperbolic_plan(model, mdp, start_state, goal_state, max_depth=3, angle_threshold=np.pi/4):\n",
    "    \"\"\"\n",
    "    Generate a hierarchical plan using hyperbolic embeddings.\n",
    "    \n",
    "    The algorithm:\n",
    "    1. Get embedding φ(start, goal)\n",
    "    2. Find lowest-norm atomic embedding φ(s', s') along radial line\n",
    "    3. If found, recursively plan (start, s') and (s', goal)\n",
    "    4. Return the hierarchical plan\n",
    "    \n",
    "    Args:\n",
    "        model: Trained hyperbolic encoder\n",
    "        mdp: MDP instance\n",
    "        start_state: Starting state\n",
    "        goal_state: Goal state\n",
    "        max_depth: Maximum recursion depth\n",
    "        angle_threshold: Angular tolerance for radial line matching\n",
    "    \n",
    "    Returns:\n",
    "        plan: List of subgoals (hierarchical decomposition)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get atomic embeddings\n",
    "    atomic_emb = get_atomic_embeddings(model, mdp)\n",
    "    \n",
    "    def plan_recursive(s1, s2, depth, visited):\n",
    "        \"\"\"Recursive planning helper.\"\"\"\n",
    "        if depth >= max_depth:\n",
    "            return [s1, s2]\n",
    "        \n",
    "        if s1 == s2:\n",
    "            return [s1]\n",
    "        \n",
    "        # Get embedding for this pair\n",
    "        pair_emb = get_embedding(model, s1, s2, mdp.n_states)\n",
    "        \n",
    "        # Find subgoal on radial line\n",
    "        subgoal, subgoal_emb = find_subgoal_on_radial_line(\n",
    "            pair_emb, atomic_emb, s1, s2, \n",
    "            angle_threshold=angle_threshold,\n",
    "            exclude_states=visited\n",
    "        )\n",
    "        \n",
    "        if subgoal is None or subgoal in visited:\n",
    "            # No valid subgoal found\n",
    "            return [s1, s2]\n",
    "        \n",
    "        # Recurse on both halves\n",
    "        visited_new = visited | {subgoal}\n",
    "        left_plan = plan_recursive(s1, subgoal, depth + 1, visited_new)\n",
    "        right_plan = plan_recursive(subgoal, s2, depth + 1, visited_new)\n",
    "        \n",
    "        # Combine plans (avoiding duplicate subgoal)\n",
    "        if right_plan and right_plan[0] == subgoal:\n",
    "            return left_plan + right_plan[1:]\n",
    "        return left_plan + right_plan\n",
    "    \n",
    "    plan = plan_recursive(start_state, goal_state, 0, set())\n",
    "    return plan\n",
    "\n",
    "\n",
    "def visualize_plan(model, mdp, start_state, goal_state, plan):\n",
    "    \"\"\"\n",
    "    Visualize the planning process on the Poincare disk.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plot_poincare_disk(ax, f\"Hyperbolic Plan: {pair_label(start_state, goal_state)}\")\n",
    "    \n",
    "    # Get atomic embeddings\n",
    "    atomic_emb = get_atomic_embeddings(model, mdp)\n",
    "    \n",
    "    # Plot all atomic embeddings in light gray\n",
    "    for s, emb in atomic_emb.items():\n",
    "        ax.scatter([emb[0]], [emb[1]], c='lightgray', s=60, zorder=2, alpha=0.7)\n",
    "        ax.annotate(pair_label(s, s), (emb[0], emb[1]), fontsize=8, alpha=0.5,\n",
    "                   xytext=(3, 3), textcoords='offset points')\n",
    "    \n",
    "    # Plot the main (start, goal) embedding\n",
    "    main_emb = get_embedding(model, start_state, goal_state, mdp.n_states)\n",
    "    ax.scatter([main_emb[0]], [main_emb[1]], c='red', s=200, marker='*', \n",
    "              edgecolors='black', linewidth=2, zorder=10,\n",
    "              label=f'Target: {pair_label(start_state, goal_state)}')\n",
    "    \n",
    "    # Draw radial line from origin through target\n",
    "    direction = main_emb / (np.linalg.norm(main_emb) + 1e-10)\n",
    "    line_end = direction * 0.95  # Almost to boundary\n",
    "    ax.plot([0, line_end[0]], [0, line_end[1]], 'r--', linewidth=2, alpha=0.5, label='Radial line')\n",
    "    \n",
    "    # Highlight plan states\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(plan)))\n",
    "    for i, state in enumerate(plan):\n",
    "        emb = atomic_emb[state]\n",
    "        ax.scatter([emb[0]], [emb[1]], c=[colors[i]], s=150, \n",
    "                  edgecolors='black', linewidth=2, zorder=5)\n",
    "        ax.annotate(f\"{i+1}:{pair_label(state, state)}\", (emb[0], emb[1]), \n",
    "                   fontsize=11, fontweight='bold',\n",
    "                   xytext=(6, 6), textcoords='offset points')\n",
    "    \n",
    "    # Draw arrows between plan states\n",
    "    for i in range(len(plan) - 1):\n",
    "        emb1 = atomic_emb[plan[i]]\n",
    "        emb2 = atomic_emb[plan[i + 1]]\n",
    "        ax.annotate('', xy=emb2, xytext=emb1,\n",
    "                   arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "    \n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hyperbolic_plan.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate and display plan from state 1 to state 6\n",
    "print(\"=\"*70)\n",
    "print(\"HYPERBOLIC PLANNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start = mdp.start_state  # State 1 (index 0)\n",
    "goal = mdp.goal_state    # State 6 (index 5)\n",
    "\n",
    "plan = hyperbolic_plan(model, mdp, start, goal, max_depth=3, angle_threshold=np.pi/3)\n",
    "\n",
    "print(f\"\\nPlan from {pair_label(start, goal)}:\")\n",
    "print(f\"  Sequence: {' -> '.join([pair_label(s, s) for s in plan])}\")\n",
    "print(f\"  States: {' -> '.join([str(s+1) for s in plan])}\")\n",
    "\n",
    "# Visualize\n",
    "visualize_plan(model, mdp, start, goal, plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analysis of Atomic Embeddings and Plans\n",
    "# =============================================================================\n",
    "\n",
    "# Get and display atomic embeddings\n",
    "print(\"=\"*70)\n",
    "print(\"ATOMIC EMBEDDINGS φ(s, s)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'State':<10} {'Embedding':<25} {'Norm':<12} {'Angle (deg)':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "atomic_emb = get_atomic_embeddings(model, mdp)\n",
    "for s in range(mdp.n_states):\n",
    "    emb = atomic_emb[s]\n",
    "    norm = hyperbolic_norm(emb)\n",
    "    angle = np.degrees(np.arctan2(emb[1], emb[0]))\n",
    "    print(f\"{pair_label(s, s):<10} ({emb[0]:+.4f}, {emb[1]:+.4f}){'':>5} {norm:<12.4f} {angle:+.1f}\")\n",
    "\n",
    "# Plot all atomic embeddings\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_poincare_disk(ax, \"All Atomic Embeddings φ(s, s)\")\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, mdp.n_states))\n",
    "for s in range(mdp.n_states):\n",
    "    emb = atomic_emb[s]\n",
    "    norm = hyperbolic_norm(emb)\n",
    "    ax.scatter([emb[0]], [emb[1]], c=[colors[s]], s=150, \n",
    "              edgecolors='black', linewidth=2, zorder=5)\n",
    "    ax.annotate(f\"{pair_label(s, s)}\\n(||·||={norm:.2f})\", (emb[0], emb[1]), \n",
    "               fontsize=10, fontweight='bold',\n",
    "               xytext=(8, 8), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('atomic_embeddings.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Generate plans from all non-goal states\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PLANS FROM ALL STATES TO GOAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for start_s in range(mdp.n_states):\n",
    "    if start_s != mdp.goal_state:\n",
    "        plan = hyperbolic_plan(model, mdp, start_s, mdp.goal_state, max_depth=3)\n",
    "        print(f\"  {pair_label(start_s, mdp.goal_state)}: {' -> '.join([str(s+1) for s in plan])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Shadow Cone Loss in Poincaré Half-Space\n",
    "\n",
    "This section implements an alternative loss function using **Shadow Cone geometry** in the **Poincaré Half-Space model**.\n",
    "\n",
    "## Key Differences from InfoNCE:\n",
    "\n",
    "| Aspect | InfoNCE (Part 1) | Shadow Cone Loss |\n",
    "|--------|------------------|------------------|\n",
    "| **Geometry** | Poincaré Ball | Poincaré Half-Space |\n",
    "| **Relation** | Symmetric similarity | Asymmetric containment |\n",
    "| **What it learns** | \"Same vs different\" | \"Contains vs doesn't contain\" |\n",
    "| **Structure** | Clusters | Hierarchy (tree-like) |\n",
    "\n",
    "## Poincaré Half-Space Model\n",
    "\n",
    "$$\\mathbb{H}^n = \\{(x_1, \\ldots, x_{n-1}, y) \\in \\mathbb{R}^n : y > 0\\}$$\n",
    "\n",
    "- Points near boundary (y → 0) are \"specific\" (leaf intervals)\n",
    "- Points higher (large y) are \"general\" (root intervals)\n",
    "- Shadow cones point downward from parent to children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Poincaré Half-Space Geometry\n",
    "# =============================================================================\n",
    "\n",
    "class HalfSpaceGeometry:\n",
    "    \"\"\"\n",
    "    Utility class for Poincaré Half-Space computations.\n",
    "    \n",
    "    The half-space model: H^n = {(x, y) : y > 0}\n",
    "    - x: horizontal coordinates (can be multi-dimensional)\n",
    "    - y: vertical coordinate (must be positive)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_y=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_y: Minimum y value for numerical stability\n",
    "        \"\"\"\n",
    "        self.min_y = min_y\n",
    "    \n",
    "    def clamp_y(self, y):\n",
    "        \"\"\"Ensure y > min_y for numerical stability.\"\"\"\n",
    "        return torch.clamp(y, min=self.min_y)\n",
    "    \n",
    "    def distance(self, p, q):\n",
    "        \"\"\"\n",
    "        Compute hyperbolic distance in the half-space model.\n",
    "        \n",
    "        d_H(p, q) = arcosh(1 + ||p - q||^2 / (2 * y_p * y_q))\n",
    "        \n",
    "        Args:\n",
    "            p: Points of shape (..., dim) where last coord is y\n",
    "            q: Points of shape (..., dim) where last coord is y\n",
    "        \n",
    "        Returns:\n",
    "            Distances of shape (...)\n",
    "        \"\"\"\n",
    "        # Extract y coordinates (last dimension)\n",
    "        y_p = self.clamp_y(p[..., -1])\n",
    "        y_q = self.clamp_y(q[..., -1])\n",
    "        \n",
    "        # Squared Euclidean distance\n",
    "        diff = p - q\n",
    "        sq_dist = (diff ** 2).sum(dim=-1)\n",
    "        \n",
    "        # Hyperbolic distance formula\n",
    "        cosh_dist = 1.0 + sq_dist / (2.0 * y_p * y_q)\n",
    "        \n",
    "        # Clamp for numerical stability (cosh_dist >= 1)\n",
    "        cosh_dist = torch.clamp(cosh_dist, min=1.0 + 1e-7)\n",
    "        \n",
    "        return torch.acosh(cosh_dist)\n",
    "    \n",
    "    def is_below(self, v, u):\n",
    "        \"\"\"\n",
    "        Check if v is below u (y_v < y_u).\n",
    "        \n",
    "        Returns:\n",
    "            Boolean tensor of shape (...)\n",
    "        \"\"\"\n",
    "        y_v = v[..., -1]\n",
    "        y_u = u[..., -1]\n",
    "        return y_v < y_u\n",
    "    \n",
    "    def horizontal_distance(self, p, q):\n",
    "        \"\"\"\n",
    "        Compute horizontal (x) distance between points.\n",
    "        \n",
    "        Args:\n",
    "            p, q: Points of shape (..., dim)\n",
    "        \n",
    "        Returns:\n",
    "            Horizontal distances of shape (...)\n",
    "        \"\"\"\n",
    "        # All coordinates except y (last one)\n",
    "        x_p = p[..., :-1]\n",
    "        x_q = q[..., :-1]\n",
    "        return torch.norm(x_p - x_q, dim=-1)\n",
    "\n",
    "\n",
    "# Test the geometry\n",
    "geom = HalfSpaceGeometry(min_y=0.01)\n",
    "\n",
    "# Test points\n",
    "p1 = torch.tensor([0.0, 1.0])  # (x=0, y=1)\n",
    "p2 = torch.tensor([0.0, 2.0])  # (x=0, y=2) - directly above\n",
    "p3 = torch.tensor([1.0, 1.0])  # (x=1, y=1) - same height, different x\n",
    "\n",
    "print(\"Half-Space Geometry Tests:\")\n",
    "print(f\"  d(p1, p2) = {geom.distance(p1, p2).item():.4f}  (vertical)\")\n",
    "print(f\"  d(p1, p3) = {geom.distance(p1, p3).item():.4f}  (horizontal)\")\n",
    "print(f\"  d(p2, p3) = {geom.distance(p2, p3).item():.4f}  (diagonal)\")\n",
    "print(f\"  p1 below p2? {geom.is_below(p1, p2).item()}\")\n",
    "print(f\"  p2 below p1? {geom.is_below(p2, p1).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Shadow Cone Geometry\n",
    "# =============================================================================\n",
    "\n",
    "class ShadowCone:\n",
    "    \"\"\"\n",
    "    Shadow Cone computations in Poincaré Half-Space.\n",
    "    \n",
    "    A shadow cone from point u contains all points v that are:\n",
    "    1. Below u (y_v < y_u)\n",
    "    2. Within angular aperture r from the vertical axis through u\n",
    "    \n",
    "    The cone boundary is a hypercycle at hyperbolic distance r from the axis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, aperture=0.1, min_y=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            aperture: Cone aperture radius r (in hyperbolic distance units)\n",
    "            min_y: Minimum y value for numerical stability\n",
    "        \"\"\"\n",
    "        self.aperture = aperture\n",
    "        self.min_y = min_y\n",
    "        self.geom = HalfSpaceGeometry(min_y=min_y)\n",
    "    \n",
    "    def cone_energy(self, u, v):\n",
    "        \"\"\"\n",
    "        Compute energy E(u, v): distance from v to cone(u).\n",
    "        \n",
    "        E(u, v) = 0 if v is inside the cone\n",
    "        E(u, v) > 0 if v is outside the cone\n",
    "        \n",
    "        Implementation:\n",
    "        - If y_v >= y_u: E = d_H(u, v) (must go through apex)\n",
    "        - If y_v < y_u: E = max(0, d_H(v, axis(u)) - r)\n",
    "        \n",
    "        For simplicity, we use:\n",
    "        E(u, v) = d_H(u, v) - r * 1[y_v < y_u]\n",
    "        \n",
    "        Args:\n",
    "            u: Parent embeddings of shape (batch, dim) or (dim,)\n",
    "            v: Child embeddings of shape (batch, dim) or (dim,)\n",
    "        \n",
    "        Returns:\n",
    "            Energy values of shape (batch,) or scalar\n",
    "        \"\"\"\n",
    "        # Compute hyperbolic distance\n",
    "        d = self.geom.distance(u, v)\n",
    "        \n",
    "        # Check if v is below u\n",
    "        is_below = self.geom.is_below(v, u).float()\n",
    "        \n",
    "        # Energy: d - r if below, d if not below\n",
    "        # When below, being in cone means d < r, so energy = max(0, d - r)\n",
    "        # But we want smooth gradients, so use: d - r * is_below\n",
    "        energy = d - self.aperture * is_below\n",
    "        \n",
    "        return energy\n",
    "    \n",
    "    def signed_cone_energy(self, u, v):\n",
    "        \"\"\"\n",
    "        Compute signed energy: negative inside cone, positive outside.\n",
    "        \n",
    "        This gives stronger gradients for points already in the cone.\n",
    "        \n",
    "        Args:\n",
    "            u: Parent embeddings of shape (batch, dim)\n",
    "            v: Child embeddings of shape (batch, dim)\n",
    "        \n",
    "        Returns:\n",
    "            Signed energy values\n",
    "        \"\"\"\n",
    "        d = self.geom.distance(u, v)\n",
    "        is_below = self.geom.is_below(v, u).float()\n",
    "        \n",
    "        # Compute distance to vertical axis through u\n",
    "        # In 2D half-space, axis is vertical line x = x_u\n",
    "        # Distance to axis in hyperbolic space approximated by horizontal distance\n",
    "        x_u = u[..., :-1]\n",
    "        x_v = v[..., :-1]\n",
    "        y_v = self.geom.clamp_y(v[..., -1])\n",
    "        \n",
    "        # Approximate hyperbolic distance to axis\n",
    "        horizontal_dist = torch.norm(x_v - x_u, dim=-1)\n",
    "        axis_dist = torch.asinh(horizontal_dist / y_v)  # Approximate\n",
    "        \n",
    "        # Signed energy\n",
    "        # If below and close to axis: negative (inside cone)\n",
    "        # If below but far from axis: positive (outside cone)\n",
    "        # If above: use distance through apex\n",
    "        \n",
    "        inside_cone = is_below * (axis_dist < self.aperture).float()\n",
    "        \n",
    "        energy = torch.where(\n",
    "            inside_cone > 0.5,\n",
    "            axis_dist - self.aperture,  # Negative if truly inside\n",
    "            d  # Distance through apex\n",
    "        )\n",
    "        \n",
    "        return energy\n",
    "    \n",
    "    def in_cone(self, u, v, strict=True):\n",
    "        \"\"\"\n",
    "        Check if v is inside the shadow cone of u.\n",
    "        \n",
    "        Args:\n",
    "            u: Parent embeddings\n",
    "            v: Child embeddings\n",
    "            strict: If True, require v strictly below u\n",
    "        \n",
    "        Returns:\n",
    "            Boolean tensor\n",
    "        \"\"\"\n",
    "        is_below = self.geom.is_below(v, u)\n",
    "        \n",
    "        if not strict:\n",
    "            is_below = is_below | (v[..., -1] == u[..., -1])\n",
    "        \n",
    "        # Check angular/horizontal proximity\n",
    "        x_u = u[..., :-1]\n",
    "        x_v = v[..., :-1]\n",
    "        y_v = self.geom.clamp_y(v[..., -1])\n",
    "        \n",
    "        horizontal_dist = torch.norm(x_v - x_u, dim=-1)\n",
    "        axis_dist = torch.asinh(horizontal_dist / y_v)\n",
    "        \n",
    "        in_angular_range = axis_dist <= self.aperture\n",
    "        \n",
    "        return is_below & in_angular_range\n",
    "\n",
    "\n",
    "# Test shadow cone\n",
    "cone = ShadowCone(aperture=0.5, min_y=0.01)\n",
    "\n",
    "# Parent at (0, 2), children at various positions\n",
    "parent = torch.tensor([0.0, 2.0])\n",
    "child_below_center = torch.tensor([0.0, 1.0])  # Directly below - should be in cone\n",
    "child_below_side = torch.tensor([0.5, 1.0])    # Below but off-center\n",
    "child_above = torch.tensor([0.0, 3.0])         # Above parent\n",
    "child_far = torch.tensor([2.0, 0.5])           # Far away\n",
    "\n",
    "print(\"\\nShadow Cone Tests (aperture=0.5):\")\n",
    "print(f\"  Parent: (0, 2)\")\n",
    "print(f\"  Child below center (0, 1): energy={cone.cone_energy(parent, child_below_center).item():.4f}, in_cone={cone.in_cone(parent, child_below_center).item()}\")\n",
    "print(f\"  Child below side (0.5, 1): energy={cone.cone_energy(parent, child_below_side).item():.4f}, in_cone={cone.in_cone(parent, child_below_side).item()}\")\n",
    "print(f\"  Child above (0, 3): energy={cone.cone_energy(parent, child_above).item():.4f}, in_cone={cone.in_cone(parent, child_above).item()}\")\n",
    "print(f\"  Child far (2, 0.5): energy={cone.cone_energy(parent, child_far).item():.4f}, in_cone={cone.in_cone(parent, child_far).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Interval Containment and Sampling\n",
    "# =============================================================================\n",
    "\n",
    "def is_contained(child_interval, parent_interval):\n",
    "    \"\"\"\n",
    "    Check if child_interval is temporally contained in parent_interval.\n",
    "    \n",
    "    [i_c, j_c] ⊂ [i_p, j_p] iff i_p <= i_c AND j_c <= j_p\n",
    "    \n",
    "    Args:\n",
    "        child_interval: (i_c, j_c) tuple\n",
    "        parent_interval: (i_p, j_p) tuple\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if child is contained in parent\n",
    "    \"\"\"\n",
    "    i_c, j_c = child_interval\n",
    "    i_p, j_p = parent_interval\n",
    "    return i_p <= i_c and j_c <= j_p\n",
    "\n",
    "\n",
    "def is_strictly_contained(child_interval, parent_interval):\n",
    "    \"\"\"\n",
    "    Check if child is strictly contained (proper subset).\n",
    "    \n",
    "    Child ⊂ Parent AND Child ≠ Parent\n",
    "    \"\"\"\n",
    "    if child_interval == parent_interval:\n",
    "        return False\n",
    "    return is_contained(child_interval, parent_interval)\n",
    "\n",
    "\n",
    "def are_incomparable(interval1, interval2):\n",
    "    \"\"\"\n",
    "    Check if two intervals are incomparable (neither contains the other).\n",
    "    \n",
    "    This includes:\n",
    "    - Partial overlap: i1 < i2 < j1 < j2 or i2 < i1 < j2 < j1\n",
    "    - Disjoint: j1 < i2 or j2 < i1\n",
    "    \"\"\"\n",
    "    return (not is_contained(interval1, interval2) and \n",
    "            not is_contained(interval2, interval1))\n",
    "\n",
    "\n",
    "def extract_all_intervals(trajectory_length):\n",
    "    \"\"\"\n",
    "    Extract all interval pairs from a trajectory of given length.\n",
    "    \n",
    "    For trajectory of length T+1 (states s_0, ..., s_T),\n",
    "    intervals are all (i, j) with 0 <= i < j <= T.\n",
    "    \n",
    "    Args:\n",
    "        trajectory_length: T+1 (number of states)\n",
    "    \n",
    "    Returns:\n",
    "        List of (i, j) tuples\n",
    "    \"\"\"\n",
    "    T = trajectory_length - 1\n",
    "    intervals = []\n",
    "    for i in range(T + 1):\n",
    "        for j in range(i + 1, T + 1):\n",
    "            intervals.append((i, j))\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def build_containment_pairs(intervals):\n",
    "    \"\"\"\n",
    "    Build all (parent, child) containment pairs from intervals.\n",
    "    \n",
    "    Returns:\n",
    "        List of (parent_interval, child_interval) tuples\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for parent in intervals:\n",
    "        for child in intervals:\n",
    "            if is_strictly_contained(child, parent):\n",
    "                pairs.append((parent, child))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def build_incomparable_pairs(intervals):\n",
    "    \"\"\"\n",
    "    Build all incomparable pairs from intervals.\n",
    "    \n",
    "    Returns:\n",
    "        List of (interval1, interval2) tuples where neither contains the other\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    n = len(intervals)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if are_incomparable(intervals[i], intervals[j]):\n",
    "                pairs.append((intervals[i], intervals[j]))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Test with a short trajectory\n",
    "test_traj_len = 4  # States s_0, s_1, s_2, s_3\n",
    "test_intervals = extract_all_intervals(test_traj_len)\n",
    "print(f\"Intervals for trajectory of length {test_traj_len}:\")\n",
    "print(f\"  All intervals ({len(test_intervals)}): {test_intervals}\")\n",
    "\n",
    "containment_pairs = build_containment_pairs(test_intervals)\n",
    "print(f\"\\nContainment pairs ({len(containment_pairs)}):\")\n",
    "for parent, child in containment_pairs[:5]:\n",
    "    print(f\"  {child} ⊂ {parent}\")\n",
    "if len(containment_pairs) > 5:\n",
    "    print(f\"  ... and {len(containment_pairs) - 5} more\")\n",
    "\n",
    "incomparable_pairs = build_incomparable_pairs(test_intervals)\n",
    "print(f\"\\nIncomparable pairs ({len(incomparable_pairs)}):\")\n",
    "for int1, int2 in incomparable_pairs[:5]:\n",
    "    print(f\"  {int1} ⊥ {int2}\")\n",
    "if len(incomparable_pairs) > 5:\n",
    "    print(f\"  ... and {len(incomparable_pairs) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Shadow Cone Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class ShadowConeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Shadow Cone contrastive learning.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - Interval embeddings for a single trajectory\n",
    "    - Positive pairs (containment relations)\n",
    "    - Negative pairs (incomparable intervals)\n",
    "    \n",
    "    Args:\n",
    "        trajectories: List of trajectory state sequences\n",
    "        n_states: Number of states in MDP\n",
    "        max_intervals_per_traj: Maximum intervals to sample per trajectory\n",
    "        max_positive_pairs: Maximum positive pairs per sample\n",
    "        max_negative_pairs: Maximum negative pairs per sample\n",
    "        seed: Random seed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        trajectories,\n",
    "        n_states=6,\n",
    "        max_intervals_per_traj=50,\n",
    "        max_positive_pairs=100,\n",
    "        max_negative_pairs=100,\n",
    "        seed=42,\n",
    "    ):\n",
    "        self.trajectories = trajectories\n",
    "        self.n_states = n_states\n",
    "        self.max_intervals = max_intervals_per_traj\n",
    "        self.max_pos = max_positive_pairs\n",
    "        self.max_neg = max_negative_pairs\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        # Filter trajectories with sufficient length\n",
    "        self.valid_indices = [i for i, t in enumerate(trajectories) if len(t) >= 3]\n",
    "        \n",
    "        print(f\"ShadowConeDataset: {len(self.valid_indices)} valid trajectories\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample: all data from one trajectory.\n",
    "        \n",
    "        Returns:\n",
    "            dict with:\n",
    "            - 'intervals': tensor of shape (n_intervals, 2) with normalized (start, end) states\n",
    "            - 'interval_indices': tensor of shape (n_intervals, 2) with time indices (i, j)\n",
    "            - 'positive_pairs': tensor of shape (n_pos, 2) with indices into intervals\n",
    "            - 'negative_pairs': tensor of shape (n_neg, 2) with indices into intervals\n",
    "        \"\"\"\n",
    "        traj_idx = self.valid_indices[idx]\n",
    "        traj = self.trajectories[traj_idx]\n",
    "        T = len(traj) - 1\n",
    "        \n",
    "        # Extract all intervals\n",
    "        all_intervals = extract_all_intervals(len(traj))\n",
    "        \n",
    "        # Subsample if too many\n",
    "        if len(all_intervals) > self.max_intervals:\n",
    "            sampled_indices = np.random.choice(len(all_intervals), self.max_intervals, replace=False)\n",
    "            intervals = [all_intervals[i] for i in sampled_indices]\n",
    "        else:\n",
    "            intervals = all_intervals\n",
    "        \n",
    "        # Build interval representations: (start_state_normalized, end_state_normalized)\n",
    "        interval_reps = []\n",
    "        interval_indices = []\n",
    "        for (i, j) in intervals:\n",
    "            start_state = traj[i]\n",
    "            end_state = traj[j]\n",
    "            # Normalize states to [0, 1]\n",
    "            start_norm = start_state / (self.n_states - 1)\n",
    "            end_norm = end_state / (self.n_states - 1)\n",
    "            interval_reps.append([start_norm, end_norm])\n",
    "            interval_indices.append([i, j])\n",
    "        \n",
    "        # Build containment pairs\n",
    "        pos_pairs = []\n",
    "        for p_idx, parent in enumerate(intervals):\n",
    "            for c_idx, child in enumerate(intervals):\n",
    "                if is_strictly_contained(child, parent):\n",
    "                    pos_pairs.append([p_idx, c_idx])  # parent_idx, child_idx\n",
    "        \n",
    "        # Subsample positive pairs\n",
    "        if len(pos_pairs) > self.max_pos:\n",
    "            sampled = np.random.choice(len(pos_pairs), self.max_pos, replace=False)\n",
    "            pos_pairs = [pos_pairs[i] for i in sampled]\n",
    "        \n",
    "        # Build incomparable pairs\n",
    "        neg_pairs = []\n",
    "        n_intervals = len(intervals)\n",
    "        for i in range(n_intervals):\n",
    "            for j in range(i + 1, n_intervals):\n",
    "                if are_incomparable(intervals[i], intervals[j]):\n",
    "                    neg_pairs.append([i, j])\n",
    "        \n",
    "        # Subsample negative pairs\n",
    "        if len(neg_pairs) > self.max_neg:\n",
    "            sampled = np.random.choice(len(neg_pairs), self.max_neg, replace=False)\n",
    "            neg_pairs = [neg_pairs[i] for i in sampled]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        intervals_tensor = torch.tensor(interval_reps, dtype=torch.float32)\n",
    "        indices_tensor = torch.tensor(interval_indices, dtype=torch.long)\n",
    "        pos_tensor = torch.tensor(pos_pairs, dtype=torch.long) if pos_pairs else torch.zeros(0, 2, dtype=torch.long)\n",
    "        neg_tensor = torch.tensor(neg_pairs, dtype=torch.long) if neg_pairs else torch.zeros(0, 2, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'intervals': intervals_tensor,\n",
    "            'interval_indices': indices_tensor,\n",
    "            'positive_pairs': pos_tensor,\n",
    "            'negative_pairs': neg_tensor,\n",
    "        }\n",
    "\n",
    "\n",
    "def shadow_cone_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for ShadowConeDataset.\n",
    "    \n",
    "    Since each trajectory has different numbers of intervals and pairs,\n",
    "    we return lists instead of stacking.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'intervals': [item['intervals'] for item in batch],\n",
    "        'interval_indices': [item['interval_indices'] for item in batch],\n",
    "        'positive_pairs': [item['positive_pairs'] for item in batch],\n",
    "        'negative_pairs': [item['negative_pairs'] for item in batch],\n",
    "    }\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "shadow_dataset = ShadowConeDataset(\n",
    "    trajectories=trajectories,\n",
    "    n_states=6,\n",
    "    max_intervals_per_traj=50,\n",
    "    max_positive_pairs=100,\n",
    "    max_negative_pairs=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Test one sample\n",
    "sample = shadow_dataset[0]\n",
    "print(f\"\\nSample from ShadowConeDataset:\")\n",
    "print(f\"  Intervals shape: {sample['intervals'].shape}\")\n",
    "print(f\"  Interval indices shape: {sample['interval_indices'].shape}\")\n",
    "print(f\"  Positive pairs: {sample['positive_pairs'].shape[0]}\")\n",
    "print(f\"  Negative pairs: {sample['negative_pairs'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Half-Space Encoder\n",
    "# =============================================================================\n",
    "\n",
    "class HalfSpaceEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode (start_state, end_state) interval pairs to Poincaré Half-Space.\n",
    "    \n",
    "    Output: (x, y) where x is unrestricted and y > 0.\n",
    "    \n",
    "    Architecture:\n",
    "    - Euclidean MLP layers\n",
    "    - Final layer outputs (x, log_y) and we apply exp to get y > 0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=128, output_dim=2, min_y=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Input dimension (2 for normalized start/end states)\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            output_dim: Output dimension (2 for 2D half-space)\n",
    "            min_y: Minimum y value for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.min_y = min_y\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize output layer for reasonable starting positions\n",
    "        # Want y to start around 0.5-1.0, so log_y around -0.7 to 0\n",
    "        with torch.no_grad():\n",
    "            self.network[-1].bias.data[-1] = 0.0  # log_y starts at 0 -> y = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, input_dim) with normalized states\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (batch, output_dim) in half-space (last dim is y > 0)\n",
    "        \"\"\"\n",
    "        out = self.network(x)\n",
    "        \n",
    "        # Split into horizontal and vertical components\n",
    "        x_coord = out[..., :-1]\n",
    "        log_y = out[..., -1:]\n",
    "        \n",
    "        # Apply softplus to ensure y > min_y\n",
    "        y = self.min_y + nn.functional.softplus(log_y)\n",
    "        \n",
    "        return torch.cat([x_coord, y], dim=-1)\n",
    "    \n",
    "    def get_embedding(self, start_state, end_state, n_states=6):\n",
    "        \"\"\"Get embedding for a single interval.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            s_norm = start_state / (n_states - 1)\n",
    "            e_norm = end_state / (n_states - 1)\n",
    "            x = torch.tensor([[s_norm, e_norm]], dtype=torch.float32, device=device)\n",
    "            return self.forward(x).squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "# Test encoder\n",
    "test_encoder = HalfSpaceEncoder(input_dim=2, hidden_dim=64, output_dim=2).to(device)\n",
    "test_input = torch.tensor([[0.0, 1.0], [0.2, 0.8], [0.4, 0.6]], dtype=torch.float32, device=device)\n",
    "test_output = test_encoder(test_input)\n",
    "\n",
    "print(\"Half-Space Encoder Test:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Sample outputs:\")\n",
    "for i in range(test_input.shape[0]):\n",
    "    x, y = test_output[i, 0].item(), test_output[i, 1].item()\n",
    "    print(f\"    Input {test_input[i].tolist()} -> (x={x:.4f}, y={y:.4f})\")\n",
    "print(f\"  All y > 0? {(test_output[:, -1] > 0).all().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Shadow Cone Loss Functions\n",
    "# =============================================================================\n",
    "\n",
    "class ShadowConeLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Shadow Cone Contrastive Loss for hierarchical embeddings.\n",
    "    \n",
    "    Loss = L_positive + L_negative + L_regularization\n",
    "    \n",
    "    - L_positive: Push children into parent cones\n",
    "    - L_negative: Push incomparable pairs apart\n",
    "    - L_regularization: Encourage spread in y-coordinate (hierarchy levels)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        aperture=0.2,\n",
    "        positive_margin=0.1,\n",
    "        negative_margin=1.0,\n",
    "        reg_weight=0.01,\n",
    "        min_y=0.01,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            aperture: Cone aperture radius r\n",
    "            positive_margin: Margin γ₁ for positive pairs\n",
    "            negative_margin: Margin γ₂ for negative pairs\n",
    "            reg_weight: Weight λ for regularization term\n",
    "            min_y: Minimum y value for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cone = ShadowCone(aperture=aperture, min_y=min_y)\n",
    "        self.positive_margin = positive_margin\n",
    "        self.negative_margin = negative_margin\n",
    "        self.reg_weight = reg_weight\n",
    "    \n",
    "    def forward(self, embeddings, positive_pairs, negative_pairs):\n",
    "        \"\"\"\n",
    "        Compute the shadow cone loss.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Tensor of shape (n_intervals, dim) - all interval embeddings\n",
    "            positive_pairs: Tensor of shape (n_pos, 2) with (parent_idx, child_idx)\n",
    "            negative_pairs: Tensor of shape (n_neg, 2) with (idx1, idx2) incomparable\n",
    "        \n",
    "        Returns:\n",
    "            Total loss and dict of component losses\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # Positive loss: push children into parent cones\n",
    "        if positive_pairs.shape[0] > 0:\n",
    "            parent_emb = embeddings[positive_pairs[:, 0]]  # (n_pos, dim)\n",
    "            child_emb = embeddings[positive_pairs[:, 1]]   # (n_pos, dim)\n",
    "            \n",
    "            # Energy: distance from child to parent's cone\n",
    "            energy = self.cone.cone_energy(parent_emb, child_emb)\n",
    "            \n",
    "            # Soft margin loss: log(1 + exp(E - γ₁))\n",
    "            pos_loss = torch.log1p(torch.exp(energy - self.positive_margin)).mean()\n",
    "            losses['positive'] = pos_loss\n",
    "        else:\n",
    "            pos_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "            losses['positive'] = pos_loss\n",
    "        \n",
    "        # Negative loss: push incomparable pairs apart\n",
    "        if negative_pairs.shape[0] > 0:\n",
    "            emb1 = embeddings[negative_pairs[:, 0]]\n",
    "            emb2 = embeddings[negative_pairs[:, 1]]\n",
    "            \n",
    "            # For negatives, neither should be in the other's cone\n",
    "            # Compute energy in both directions\n",
    "            energy_12 = self.cone.cone_energy(emb1, emb2)  # emb2 in cone of emb1?\n",
    "            energy_21 = self.cone.cone_energy(emb2, emb1)  # emb1 in cone of emb2?\n",
    "            \n",
    "            # Take minimum: we want BOTH energies to be large\n",
    "            # If either is small, it's a violation\n",
    "            min_energy = torch.min(energy_12, energy_21)\n",
    "            \n",
    "            # Soft margin loss: log(1 + exp(γ₂ - E))\n",
    "            neg_loss = torch.log1p(torch.exp(self.negative_margin - min_energy)).mean()\n",
    "            losses['negative'] = neg_loss\n",
    "        else:\n",
    "            neg_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "            losses['negative'] = neg_loss\n",
    "        \n",
    "        # Regularization: encourage spread in y-coordinate\n",
    "        y_coords = embeddings[:, -1]\n",
    "        if y_coords.shape[0] > 1:\n",
    "            y_var = y_coords.var()\n",
    "            reg_loss = self.reg_weight / (y_var + 1e-6)\n",
    "            losses['regularization'] = reg_loss\n",
    "        else:\n",
    "            reg_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "            losses['regularization'] = reg_loss\n",
    "        \n",
    "        total_loss = pos_loss + neg_loss + reg_loss\n",
    "        losses['total'] = total_loss\n",
    "        \n",
    "        return total_loss, losses\n",
    "\n",
    "\n",
    "class ShadowConeInfoNCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    InfoNCE-style Shadow Cone Loss.\n",
    "    \n",
    "    For each parent with child and negatives:\n",
    "    L = -log(exp(-E(parent, child)) / (exp(-E(parent, child)) + Σ exp(-E(parent, neg))))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, aperture=0.2, temperature=0.5, min_y=0.01):\n",
    "        super().__init__()\n",
    "        self.cone = ShadowCone(aperture=aperture, min_y=min_y)\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, embeddings, positive_pairs, negative_pairs):\n",
    "        \"\"\"\n",
    "        Compute InfoNCE-style shadow cone loss.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Tensor of shape (n_intervals, dim)\n",
    "            positive_pairs: Tensor of shape (n_pos, 2) with (parent_idx, child_idx)\n",
    "            negative_pairs: Tensor of shape (n_neg, 2) with incomparable pairs\n",
    "        \n",
    "        Returns:\n",
    "            Loss and component dict\n",
    "        \"\"\"\n",
    "        if positive_pairs.shape[0] == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device), {'total': 0.0}\n",
    "        \n",
    "        parent_emb = embeddings[positive_pairs[:, 0]]\n",
    "        child_emb = embeddings[positive_pairs[:, 1]]\n",
    "        \n",
    "        # Positive energies\n",
    "        pos_energy = self.cone.cone_energy(parent_emb, child_emb)\n",
    "        pos_score = -pos_energy / self.temperature\n",
    "        \n",
    "        # For negatives, sample from incomparable pairs\n",
    "        if negative_pairs.shape[0] > 0:\n",
    "            # For each positive pair, get negative scores\n",
    "            # Use all negative interval pairs as negatives\n",
    "            neg_emb = embeddings[negative_pairs[:, 1]]  # Use second element as negative\n",
    "            \n",
    "            # Expand for broadcasting: (n_pos, 1, dim) and (1, n_neg, dim)\n",
    "            parent_expanded = parent_emb.unsqueeze(1)\n",
    "            neg_expanded = neg_emb.unsqueeze(0)\n",
    "            \n",
    "            # Compute energies: (n_pos, n_neg)\n",
    "            # Need to handle shapes carefully\n",
    "            n_pos = parent_emb.shape[0]\n",
    "            n_neg = neg_emb.shape[0]\n",
    "            \n",
    "            neg_energies = torch.zeros(n_pos, n_neg, device=embeddings.device)\n",
    "            for i in range(n_pos):\n",
    "                neg_energies[i] = self.cone.cone_energy(\n",
    "                    parent_emb[i:i+1].expand(n_neg, -1),\n",
    "                    neg_emb\n",
    "                )\n",
    "            \n",
    "            neg_scores = -neg_energies / self.temperature\n",
    "            \n",
    "            # InfoNCE loss\n",
    "            all_scores = torch.cat([pos_score.unsqueeze(1), neg_scores], dim=1)\n",
    "            labels = torch.zeros(n_pos, dtype=torch.long, device=embeddings.device)\n",
    "            loss = nn.functional.cross_entropy(all_scores, labels)\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=embeddings.device)\n",
    "        \n",
    "        return loss, {'total': loss.item() if isinstance(loss, torch.Tensor) else loss}\n",
    "\n",
    "\n",
    "# Test loss functions\n",
    "print(\"Testing Shadow Cone Loss:\")\n",
    "test_emb = torch.tensor([\n",
    "    [0.0, 2.0],   # High y - parent\n",
    "    [0.1, 1.0],   # Below, close to axis - child (in cone)\n",
    "    [1.0, 0.5],   # Below, far from axis - negative\n",
    "    [0.0, 3.0],   # Above - negative\n",
    "], dtype=torch.float32)\n",
    "\n",
    "test_pos = torch.tensor([[0, 1]], dtype=torch.long)  # parent=0, child=1\n",
    "test_neg = torch.tensor([[0, 2], [1, 3]], dtype=torch.long)  # incomparable pairs\n",
    "\n",
    "loss_fn = ShadowConeLoss(aperture=0.3, positive_margin=0.1, negative_margin=0.5)\n",
    "loss, components = loss_fn(test_emb, test_pos, test_neg)\n",
    "\n",
    "print(f\"  Total loss: {loss.item():.4f}\")\n",
    "for name, val in components.items():\n",
    "    if isinstance(val, torch.Tensor):\n",
    "        print(f\"  {name}: {val.item():.4f}\")\n",
    "    else:\n",
    "        print(f\"  {name}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training with Shadow Cone Loss\n",
    "# =============================================================================\n",
    "\n",
    "def train_shadow_cone_model(\n",
    "    encoder,\n",
    "    dataset,\n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    aperture=0.2,\n",
    "    positive_margin=0.1,\n",
    "    negative_margin=1.0,\n",
    "    reg_weight=0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the half-space encoder with shadow cone loss.\n",
    "    \n",
    "    Args:\n",
    "        encoder: HalfSpaceEncoder model\n",
    "        dataset: ShadowConeDataset\n",
    "        num_epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        aperture: Cone aperture parameter\n",
    "        positive_margin: Margin for positive pairs\n",
    "        negative_margin: Margin for negative pairs\n",
    "        reg_weight: Regularization weight\n",
    "    \n",
    "    Returns:\n",
    "        Training history (list of dicts)\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-5)\n",
    "    \n",
    "    loss_fn = ShadowConeLoss(\n",
    "        aperture=aperture,\n",
    "        positive_margin=positive_margin,\n",
    "        negative_margin=negative_margin,\n",
    "        reg_weight=reg_weight,\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=1,  # Process one trajectory at a time\n",
    "        shuffle=True,\n",
    "        collate_fn=shadow_cone_collate_fn\n",
    "    )\n",
    "    \n",
    "    history = []\n",
    "    encoder.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = {'total': 0, 'positive': 0, 'negative': 0, 'regularization': 0}\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # Process each trajectory in the batch\n",
    "            for intervals, pos_pairs, neg_pairs in zip(\n",
    "                batch['intervals'], batch['positive_pairs'], batch['negative_pairs']\n",
    "            ):\n",
    "                if intervals.shape[0] < 2:\n",
    "                    continue\n",
    "                \n",
    "                intervals = intervals.to(device)\n",
    "                pos_pairs = pos_pairs.to(device)\n",
    "                neg_pairs = neg_pairs.to(device)\n",
    "                \n",
    "                # Forward pass: encode all intervals\n",
    "                embeddings = encoder(intervals)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, components = loss_fn(embeddings, pos_pairs, neg_pairs)\n",
    "                \n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track losses\n",
    "                for key in epoch_losses:\n",
    "                    if key in components:\n",
    "                        val = components[key]\n",
    "                        if isinstance(val, torch.Tensor):\n",
    "                            epoch_losses[key] += val.item()\n",
    "                        else:\n",
    "                            epoch_losses[key] += val\n",
    "                n_batches += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Average losses\n",
    "        if n_batches > 0:\n",
    "            for key in epoch_losses:\n",
    "                epoch_losses[key] /= n_batches\n",
    "        \n",
    "        history.append(epoch_losses)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "                  f\"total={epoch_losses['total']:.4f}, \"\n",
    "                  f\"pos={epoch_losses['positive']:.4f}, \"\n",
    "                  f\"neg={epoch_losses['negative']:.4f}, \"\n",
    "                  f\"reg={epoch_losses['regularization']:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Train the shadow cone model\n",
    "print(\"=\"*70)\n",
    "print(\"Training Shadow Cone Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "shadow_encoder = HalfSpaceEncoder(\n",
    "    input_dim=2, \n",
    "    hidden_dim=128, \n",
    "    output_dim=2,\n",
    "    min_y=0.01\n",
    ").to(device)\n",
    "\n",
    "shadow_history = train_shadow_cone_model(\n",
    "    shadow_encoder,\n",
    "    shadow_dataset,\n",
    "    num_epochs=150,\n",
    "    lr=0.001,\n",
    "    aperture=0.3,\n",
    "    positive_margin=0.1,\n",
    "    negative_margin=0.8,\n",
    "    reg_weight=0.005,\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "losses_dict = {k: [h[k] for h in shadow_history] for k in ['total', 'positive', 'negative', 'regularization']}\n",
    "\n",
    "for ax, (name, values) in zip(axes, losses_dict.items()):\n",
    "    ax.plot(values)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'{name.capitalize()} Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shadow_cone_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Half-Space Visualization\n",
    "# =============================================================================\n",
    "\n",
    "def plot_half_space(ax, title=\"\", y_max=3.0):\n",
    "    \"\"\"Draw half-space boundary and setup axes.\"\"\"\n",
    "    ax.axhline(y=0, color='black', linewidth=2, linestyle='-')\n",
    "    ax.fill_between([-3, 3], 0, -0.5, color='lightgray', alpha=0.5)\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-0.2, y_max)\n",
    "    ax.set_xlabel('x (horizontal)')\n",
    "    ax.set_ylabel('y (height = generality)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "\n",
    "\n",
    "def visualize_half_space_embeddings(encoder, mdp, hitting_stats):\n",
    "    \"\"\"\n",
    "    Visualize embeddings for (state, goal) pairs in half-space.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Get embeddings for all (state, final_goal) pairs\n",
    "    embeddings = {}\n",
    "    for state in range(mdp.n_states):\n",
    "        if state != mdp.goal_state:\n",
    "            emb = encoder.get_embedding(state, mdp.goal_state, mdp.n_states)\n",
    "            embeddings[state] = emb\n",
    "    \n",
    "    states = sorted(embeddings.keys())\n",
    "    coords = np.array([embeddings[s] for s in states])\n",
    "    means = np.array([hitting_stats[s]['mean'] for s in states])\n",
    "    vars_ = np.array([hitting_stats[s]['var'] for s in states])\n",
    "    \n",
    "    # Determine y_max for all plots\n",
    "    y_max = coords[:, 1].max() * 1.3\n",
    "    \n",
    "    # Plot 1: Colored by mean hitting time\n",
    "    ax = axes[0]\n",
    "    plot_half_space(ax, \"Colored by Mean Hitting Time\", y_max)\n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=means, cmap='viridis',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Mean Hitting Time')\n",
    "    \n",
    "    for s in states:\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=11, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Plot 2: Colored by variance\n",
    "    ax = axes[1]\n",
    "    plot_half_space(ax, \"Colored by Variance\", y_max)\n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=vars_, cmap='plasma',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Variance')\n",
    "    \n",
    "    for s in states:\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=11, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Plot 3: Show shadow cones\n",
    "    ax = axes[2]\n",
    "    plot_half_space(ax, \"Shadow Cone Structure\", y_max)\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(states)))\n",
    "    for i, s in enumerate(states):\n",
    "        emb = embeddings[s]\n",
    "        ax.scatter([emb[0]], [emb[1]], c=[colors[i]], s=150,\n",
    "                  edgecolors='black', linewidth=2, zorder=5,\n",
    "                  label=f\"{pair_label(s, mdp.goal_state)}\")\n",
    "        \n",
    "        # Draw shadow cone (simplified as triangle)\n",
    "        cone_aperture = 0.3\n",
    "        y_bottom = 0.01\n",
    "        width_at_bottom = (emb[1] - y_bottom) * np.sinh(cone_aperture)\n",
    "        \n",
    "        triangle = plt.Polygon([\n",
    "            [emb[0], emb[1]],\n",
    "            [emb[0] - width_at_bottom, y_bottom],\n",
    "            [emb[0] + width_at_bottom, y_bottom]\n",
    "        ], alpha=0.2, color=colors[i])\n",
    "        ax.add_patch(triangle)\n",
    "        \n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=11, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('half_space_embeddings.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Visualize\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Half-Space Embeddings Visualization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "shadow_embeddings = visualize_half_space_embeddings(shadow_encoder, mdp, hitting_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Shadow Cone Analysis and Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_shadow_cone_embeddings(encoder, mdp, hitting_stats):\n",
    "    \"\"\"\n",
    "    Analyze the learned half-space embeddings.\n",
    "    \n",
    "    Expected structure:\n",
    "    - y-coordinate should correlate with interval \"size\" (generality)\n",
    "    - Larger intervals should have higher y (more general)\n",
    "    - States with high variance should have lower y (more specific)\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    \n",
    "    # Get embeddings for all (state, goal) pairs\n",
    "    embeddings = {}\n",
    "    for s in range(mdp.n_states):\n",
    "        if s != mdp.goal_state:\n",
    "            emb = encoder.get_embedding(s, mdp.goal_state, mdp.n_states)\n",
    "            embeddings[s] = emb\n",
    "    \n",
    "    states = sorted(embeddings.keys())\n",
    "    \n",
    "    # Extract coordinates\n",
    "    x_coords = np.array([embeddings[s][0] for s in states])\n",
    "    y_coords = np.array([embeddings[s][1] for s in states])\n",
    "    means = np.array([hitting_stats[s]['mean'] for s in states])\n",
    "    vars_ = np.array([hitting_stats[s]['var'] for s in states])\n",
    "    \n",
    "    # Compute correlations\n",
    "    # Hypothesis: y ~ 1/variance (more specific = lower y, higher variance)\n",
    "    # Hypothesis: x ~ mean (similar means = similar x)\n",
    "    \n",
    "    corr_y_var, p_y_var = stats.spearmanr(y_coords, vars_)\n",
    "    corr_y_mean, p_y_mean = stats.spearmanr(y_coords, means)\n",
    "    corr_x_mean, p_x_mean = stats.spearmanr(x_coords, means)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"SHADOW CONE EMBEDDING ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nExpected structure in Half-Space:\")\n",
    "    print(\"  - High y = general (large intervals)\")\n",
    "    print(\"  - Low y = specific (small intervals / high variance)\")\n",
    "    print(\"  - x separates different 'branches' of the hierarchy\")\n",
    "    \n",
    "    print(\"\\nCorrelations:\")\n",
    "    print(f\"  y vs variance:  ρ = {corr_y_var:.4f} (p = {p_y_var:.4f})\")\n",
    "    print(f\"  y vs mean:      ρ = {corr_y_mean:.4f} (p = {p_y_mean:.4f})\")\n",
    "    print(f\"  x vs mean:      ρ = {corr_x_mean:.4f} (p = {p_x_mean:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATE-BY-STATE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Pair':<10} {'x':<12} {'y':<12} {'Mean T':<10} {'Var T':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for s in states:\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        print(f\"{label:<10} {emb[0]:<12.4f} {emb[1]:<12.4f} \"\n",
    "              f\"{hitting_stats[s]['mean']:<10.2f} {hitting_stats[s]['var']:<10.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'embeddings': embeddings,\n",
    "        'corr_y_var': corr_y_var,\n",
    "        'corr_y_mean': corr_y_mean,\n",
    "        'corr_x_mean': corr_x_mean,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_containment_accuracy(encoder, trajectories, mdp, n_samples=100):\n",
    "    \"\"\"\n",
    "    Evaluate whether containment relationships are correctly embedded.\n",
    "    \n",
    "    For (parent, child) pairs where child ⊂ parent:\n",
    "    - child should be in parent's shadow cone (y_child < y_parent and close to axis)\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    cone = ShadowCone(aperture=0.3, min_y=0.01)\n",
    "    \n",
    "    correct_pos = 0\n",
    "    total_pos = 0\n",
    "    correct_neg = 0\n",
    "    total_neg = 0\n",
    "    \n",
    "    # Sample trajectories\n",
    "    np.random.seed(42)\n",
    "    sampled_trajs = np.random.choice(len(trajectories), min(n_samples, len(trajectories)), replace=False)\n",
    "    \n",
    "    for traj_idx in sampled_trajs:\n",
    "        traj = trajectories[traj_idx]\n",
    "        if len(traj) < 3:\n",
    "            continue\n",
    "        \n",
    "        intervals = extract_all_intervals(len(traj))\n",
    "        if len(intervals) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            interval_emb = []\n",
    "            for (i, j) in intervals:\n",
    "                s_norm = traj[i] / (mdp.n_states - 1)\n",
    "                e_norm = traj[j] / (mdp.n_states - 1)\n",
    "                x = torch.tensor([[s_norm, e_norm]], dtype=torch.float32, device=device)\n",
    "                emb = encoder(x).squeeze(0)\n",
    "                interval_emb.append(emb)\n",
    "            interval_emb = torch.stack(interval_emb)\n",
    "        \n",
    "        # Check containment pairs (sample subset)\n",
    "        pos_pairs = build_containment_pairs(intervals)\n",
    "        if len(pos_pairs) > 20:\n",
    "            pos_indices = np.random.choice(len(pos_pairs), 20, replace=False)\n",
    "            pos_pairs = [pos_pairs[i] for i in pos_indices]\n",
    "        \n",
    "        for parent_int, child_int in pos_pairs:\n",
    "            parent_idx = intervals.index(parent_int)\n",
    "            child_idx = intervals.index(child_int)\n",
    "            \n",
    "            parent_emb = interval_emb[parent_idx]\n",
    "            child_emb = interval_emb[child_idx]\n",
    "            \n",
    "            in_cone = cone.in_cone(parent_emb, child_emb).item()\n",
    "            if in_cone:\n",
    "                correct_pos += 1\n",
    "            total_pos += 1\n",
    "        \n",
    "        # Check incomparable pairs (sample subset)\n",
    "        neg_pairs = build_incomparable_pairs(intervals)\n",
    "        if len(neg_pairs) > 20:\n",
    "            neg_indices = np.random.choice(len(neg_pairs), 20, replace=False)\n",
    "            neg_pairs = [neg_pairs[i] for i in neg_indices]\n",
    "        \n",
    "        for int1, int2 in neg_pairs:\n",
    "            idx1 = intervals.index(int1)\n",
    "            idx2 = intervals.index(int2)\n",
    "            \n",
    "            emb1 = interval_emb[idx1]\n",
    "            emb2 = interval_emb[idx2]\n",
    "            \n",
    "            # Neither should be in the other's cone\n",
    "            not_in_1 = not cone.in_cone(emb1, emb2).item()\n",
    "            not_in_2 = not cone.in_cone(emb2, emb1).item()\n",
    "            \n",
    "            if not_in_1 and not_in_2:\n",
    "                correct_neg += 1\n",
    "            total_neg += 1\n",
    "    \n",
    "    pos_accuracy = correct_pos / total_pos if total_pos > 0 else 0\n",
    "    neg_accuracy = correct_neg / total_neg if total_neg > 0 else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONTAINMENT EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Positive pairs (child should be in parent's cone):\")\n",
    "    print(f\"  Accuracy: {pos_accuracy*100:.1f}% ({correct_pos}/{total_pos})\")\n",
    "    print(f\"\\nNegative pairs (neither should be in the other's cone):\")\n",
    "    print(f\"  Accuracy: {neg_accuracy*100:.1f}% ({correct_neg}/{total_neg})\")\n",
    "    \n",
    "    return {\n",
    "        'positive_accuracy': pos_accuracy,\n",
    "        'negative_accuracy': neg_accuracy,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "shadow_analysis = analyze_shadow_cone_embeddings(shadow_encoder, mdp, hitting_stats)\n",
    "\n",
    "# Evaluate containment accuracy\n",
    "containment_results = evaluate_containment_accuracy(shadow_encoder, trajectories, mdp, n_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Comparison: InfoNCE vs Shadow Cone\n",
    "# =============================================================================\n",
    "\n",
    "def plot_comparison(poincare_embeddings, half_space_embeddings, hitting_stats, mdp):\n",
    "    \"\"\"\n",
    "    Side-by-side comparison of Poincaré Ball (InfoNCE) and Half-Space (Shadow Cone) embeddings.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    states = sorted(poincare_embeddings.keys())\n",
    "    \n",
    "    # Poincaré Ball embeddings\n",
    "    ax = axes[0]\n",
    "    plot_poincare_disk(ax, \"Poincaré Ball (InfoNCE Loss)\")\n",
    "    \n",
    "    coords = np.array([poincare_embeddings[s] for s in states])\n",
    "    vars_ = np.array([hitting_stats[s]['var'] for s in states])\n",
    "    \n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=vars_, cmap='plasma_r',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Variance', shrink=0.8)\n",
    "    \n",
    "    for s in states:\n",
    "        emb = poincare_embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=11, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Half-Space embeddings\n",
    "    ax = axes[1]\n",
    "    hs_coords = np.array([half_space_embeddings[s] for s in states])\n",
    "    y_max = hs_coords[:, 1].max() * 1.3\n",
    "    \n",
    "    plot_half_space(ax, \"Half-Space (Shadow Cone Loss)\", y_max)\n",
    "    \n",
    "    scatter = ax.scatter(hs_coords[:, 0], hs_coords[:, 1], c=vars_, cmap='plasma_r',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Variance', shrink=0.8)\n",
    "    \n",
    "    for s in states:\n",
    "        emb = half_space_embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=11, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('embedding_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Compare the two approaches\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: InfoNCE vs Shadow Cone Loss\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get Poincaré Ball embeddings from the first model\n",
    "poincare_emb = get_state_embeddings(model, mdp)\n",
    "\n",
    "# Get Half-Space embeddings from the shadow cone model\n",
    "half_space_emb = {s: shadow_embeddings[s] for s in shadow_embeddings}\n",
    "\n",
    "plot_comparison(poincare_emb, half_space_emb, hitting_stats, mdp)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n--- Poincaré Ball (InfoNCE) ---\")\n",
    "print(f\"  Norm vs Variance correlation: {results['corr_norm_var']:.4f}\")\n",
    "print(f\"  Angle vs Mean correlation:    {results['corr_angle_mean']:.4f}\")\n",
    "\n",
    "print(\"\\n--- Half-Space (Shadow Cone) ---\")\n",
    "print(f\"  y vs Variance correlation:    {shadow_analysis['corr_y_var']:.4f}\")\n",
    "print(f\"  y vs Mean correlation:        {shadow_analysis['corr_y_mean']:.4f}\")\n",
    "print(f\"  Containment accuracy:         {containment_results['positive_accuracy']*100:.1f}%\")\n",
    "print(f\"  Separation accuracy:          {containment_results['negative_accuracy']*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY DIFFERENCES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "| Aspect            | InfoNCE (Ball)                | Shadow Cone (Half-Space)      |\n",
    "|-------------------|-------------------------------|-------------------------------|\n",
    "| Geometry          | Poincaré Ball                 | Poincaré Half-Space           |\n",
    "| Loss function     | Symmetric contrastive         | Asymmetric containment        |\n",
    "| Learns            | \"Same vs different\" clusters  | Hierarchical containment      |\n",
    "| Structure         | Radial (norm/angle)           | Vertical hierarchy (y)        |\n",
    "| Interpretability  | Norm ~ variance, angle ~ mean | y ~ generality, x ~ branch    |\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Shadow Cone Loss in Poincaré Ball\n",
    "\n",
    "This section implements the **Shadow Cone Loss** in the **Poincaré Ball** model.\n",
    "\n",
    "## Key Differences from Half-Space Shadow Cone:\n",
    "\n",
    "| Aspect | Half-Space | Poincaré Ball |\n",
    "|--------|------------|---------------|\n",
    "| **Most general** | High y (top) | Origin (norm ≈ 0) |\n",
    "| **Most specific** | y → 0 (boundary) | Boundary (norm → 1) |\n",
    "| **Parent-child** | Parent has higher y | Parent has lower norm |\n",
    "| **Cone direction** | Downward | Outward from origin |\n",
    "\n",
    "## Hierarchy Convention\n",
    "\n",
    "- **Origin** (norm = 0): most general/abstract (root intervals)\n",
    "- **Boundary** (norm → 1): most specific/concrete (leaf intervals)\n",
    "- **Parents** have **lower norm** than children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Poincaré Ball Geometry (for Shadow Cone)\n",
    "# =============================================================================\n",
    "\n",
    "class PoincareBallGeometry:\n",
    "    \"\"\"\n",
    "    Utility class for Poincaré Ball computations.\n",
    "    \n",
    "    The ball model: B^n = {x ∈ R^n : ||x|| < 1}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps=1e-5, max_norm=0.95):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eps: Small constant for numerical stability\n",
    "            max_norm: Maximum allowed norm (keep away from boundary)\n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "        self.max_norm = max_norm\n",
    "    \n",
    "    def distance(self, u, v):\n",
    "        \"\"\"\n",
    "        Compute hyperbolic distance in the Poincaré ball.\n",
    "        \n",
    "        d_B(u, v) = arcosh(1 + 2 * ||u - v||^2 / ((1 - ||u||^2)(1 - ||v||^2)))\n",
    "        \n",
    "        Args:\n",
    "            u, v: Points of shape (..., dim)\n",
    "        \n",
    "        Returns:\n",
    "            Distances of shape (...)\n",
    "        \"\"\"\n",
    "        diff_norm_sq = torch.sum((u - v)**2, dim=-1)\n",
    "        u_norm_sq = torch.sum(u**2, dim=-1)\n",
    "        v_norm_sq = torch.sum(v**2, dim=-1)\n",
    "        \n",
    "        # Clamp norms to avoid numerical issues at boundary\n",
    "        u_norm_sq = torch.clamp(u_norm_sq, max=self.max_norm**2)\n",
    "        v_norm_sq = torch.clamp(v_norm_sq, max=self.max_norm**2)\n",
    "        \n",
    "        denom = (1 - u_norm_sq) * (1 - v_norm_sq)\n",
    "        denom = torch.clamp(denom, min=self.eps)\n",
    "        \n",
    "        arg = 1 + 2 * diff_norm_sq / denom\n",
    "        arg = torch.clamp(arg, min=1 + self.eps)\n",
    "        \n",
    "        return torch.acosh(arg)\n",
    "    \n",
    "    def norm(self, x):\n",
    "        \"\"\"Compute Euclidean norm.\"\"\"\n",
    "        return torch.norm(x, dim=-1)\n",
    "    \n",
    "    def project_to_ball(self, x):\n",
    "        \"\"\"\n",
    "        Project points to inside the ball (norm < max_norm).\n",
    "        \n",
    "        Uses smooth tanh projection.\n",
    "        \"\"\"\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True)\n",
    "        # Smooth projection: tanh maps R -> (-1, 1)\n",
    "        scale = torch.tanh(norm) * self.max_norm / (norm + self.eps)\n",
    "        return x * scale\n",
    "    \n",
    "    def clip_to_ball(self, x):\n",
    "        \"\"\"\n",
    "        Hard clip to ball (simpler, but less smooth gradients).\n",
    "        \"\"\"\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True)\n",
    "        scale = torch.clamp(self.max_norm / (norm + self.eps), max=1.0)\n",
    "        return x * scale\n",
    "    \n",
    "    def cosine_similarity(self, u, v):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between vectors.\n",
    "        \"\"\"\n",
    "        u_norm = torch.norm(u, dim=-1)\n",
    "        v_norm = torch.norm(v, dim=-1)\n",
    "        dot = torch.sum(u * v, dim=-1)\n",
    "        return dot / (u_norm * v_norm + self.eps)\n",
    "    \n",
    "    def safe_normalize(self, x):\n",
    "        \"\"\"Normalize vector, handling zero vectors.\"\"\"\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True)\n",
    "        return x / (norm + self.eps)\n",
    "\n",
    "\n",
    "# Test the geometry\n",
    "ball_geom = PoincareBallGeometry()\n",
    "\n",
    "# Test points\n",
    "p1 = torch.tensor([0.0, 0.0])   # Origin\n",
    "p2 = torch.tensor([0.5, 0.0])   # Mid-radius\n",
    "p3 = torch.tensor([0.9, 0.0])   # Near boundary\n",
    "p4 = torch.tensor([0.0, 0.5])   # Different angle\n",
    "\n",
    "print(\"Poincaré Ball Geometry Tests:\")\n",
    "print(f\"  d(origin, mid) = {ball_geom.distance(p1, p2).item():.4f}\")\n",
    "print(f\"  d(origin, near_boundary) = {ball_geom.distance(p1, p3).item():.4f}\")\n",
    "print(f\"  d(mid, near_boundary) = {ball_geom.distance(p2, p3).item():.4f}\")\n",
    "print(f\"  d(mid_x, mid_y) = {ball_geom.distance(p2, p4).item():.4f}\")\n",
    "print(f\"  cos(p2, p4) = {ball_geom.cosine_similarity(p2, p4).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Shadow Cone in Poincaré Ball\n",
    "# =============================================================================\n",
    "\n",
    "class BallShadowCone:\n",
    "    \"\"\"\n",
    "    Shadow Cone computations in Poincaré Ball.\n",
    "    \n",
    "    Imagine a point light source at the origin. Each point u is an opaque ball.\n",
    "    The shadow cone of u contains all points \"behind\" u from the origin's perspective.\n",
    "    \n",
    "    For child v to be in parent u's shadow cone:\n",
    "    1. ||v|| > ||u|| (child has higher norm - further from origin)\n",
    "    2. v is angularly close to u (within cone aperture)\n",
    "    \n",
    "    Convention:\n",
    "    - Origin (low norm) = general/abstract (parents)\n",
    "    - Boundary (high norm) = specific/concrete (children)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 cone_angle_deg=30.0,\n",
    "                 radial_margin=0.05,\n",
    "                 angular_weight=1.0,\n",
    "                 eps=1e-5,\n",
    "                 max_norm=0.95):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cone_angle_deg: Maximum half-angle of cone in degrees\n",
    "            radial_margin: Margin for radial ordering\n",
    "            angular_weight: Weight for angular component in energy\n",
    "            eps: Small constant for numerical stability\n",
    "            max_norm: Maximum allowed norm\n",
    "        \"\"\"\n",
    "        self.cone_angle_rad = np.radians(cone_angle_deg)\n",
    "        self.cos_threshold = np.cos(self.cone_angle_rad)\n",
    "        self.radial_margin = radial_margin\n",
    "        self.angular_weight = angular_weight\n",
    "        self.eps = eps\n",
    "        self.geom = PoincareBallGeometry(eps=eps, max_norm=max_norm)\n",
    "    \n",
    "    def radial_energy(self, u, v):\n",
    "        \"\"\"\n",
    "        Compute radial component of energy.\n",
    "        \n",
    "        For containment (u=parent, v=child), we want ||v|| > ||u||.\n",
    "        Energy is high if this is violated.\n",
    "        \n",
    "        E_radial = ReLU(||u|| - ||v|| + margin)\n",
    "        \n",
    "        Args:\n",
    "            u: Parent embeddings (should have lower norm)\n",
    "            v: Child embeddings (should have higher norm)\n",
    "        \n",
    "        Returns:\n",
    "            Radial energy (0 if constraint satisfied with margin)\n",
    "        \"\"\"\n",
    "        u_norm = self.geom.norm(u)\n",
    "        v_norm = self.geom.norm(v)\n",
    "        \n",
    "        # Want v_norm > u_norm + margin\n",
    "        # Violation: u_norm - v_norm + margin > 0\n",
    "        return torch.relu(u_norm - v_norm + self.radial_margin)\n",
    "    \n",
    "    def angular_energy(self, u, v):\n",
    "        \"\"\"\n",
    "        Compute angular component of energy.\n",
    "        \n",
    "        For containment, v should be angularly aligned with u.\n",
    "        cos(angle(u, v)) should be >= cos_threshold\n",
    "        \n",
    "        E_angular = ReLU(cos_threshold - cos(angle(u, v)))\n",
    "        \n",
    "        Args:\n",
    "            u: Parent embeddings\n",
    "            v: Child embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Angular energy (0 if v is within cone angle of u)\n",
    "        \"\"\"\n",
    "        cos_sim = self.geom.cosine_similarity(u, v)\n",
    "        \n",
    "        # Want cos_sim >= cos_threshold\n",
    "        # Violation: cos_threshold - cos_sim > 0\n",
    "        return torch.relu(self.cos_threshold - cos_sim)\n",
    "    \n",
    "    def cone_energy(self, u, v):\n",
    "        \"\"\"\n",
    "        Compute total energy for (parent, child) pair.\n",
    "        \n",
    "        E(u, v) = E_radial + λ * E_angular\n",
    "        \n",
    "        Low energy means v is properly inside u's shadow cone.\n",
    "        \n",
    "        Args:\n",
    "            u: Parent embeddings\n",
    "            v: Child embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Total cone energy\n",
    "        \"\"\"\n",
    "        e_radial = self.radial_energy(u, v)\n",
    "        e_angular = self.angular_energy(u, v)\n",
    "        \n",
    "        return e_radial + self.angular_weight * e_angular\n",
    "    \n",
    "    def hyperbolic_cone_energy(self, u, v, aperture=0.3):\n",
    "        \"\"\"\n",
    "        Alternative energy using hyperbolic distance to geodesic.\n",
    "        \n",
    "        More faithful to the hyperbolic geometry but more complex.\n",
    "        \n",
    "        Args:\n",
    "            u: Parent embeddings\n",
    "            v: Child embeddings\n",
    "            aperture: Cone aperture in hyperbolic distance units\n",
    "        \n",
    "        Returns:\n",
    "            Energy based on hyperbolic distance to geodesic axis\n",
    "        \"\"\"\n",
    "        u_norm = self.geom.norm(u)\n",
    "        v_norm = self.geom.norm(v)\n",
    "        \n",
    "        # Radial check\n",
    "        radial_violation = torch.relu(u_norm - v_norm + self.radial_margin)\n",
    "        \n",
    "        # For points where radial is satisfied, check angular alignment\n",
    "        # Project v onto the geodesic through u (which is the ray from origin through u)\n",
    "        u_normalized = self.geom.safe_normalize(u)\n",
    "        \n",
    "        # Projection of v onto ray through u\n",
    "        # v_parallel = (v · û) * û\n",
    "        dot_product = torch.sum(v * u_normalized, dim=-1, keepdim=True)\n",
    "        v_parallel = dot_product * u_normalized\n",
    "        \n",
    "        # Distance from v to its projection on the axis\n",
    "        d_to_axis = self.geom.distance(v, v_parallel)\n",
    "        \n",
    "        # Angular violation: d_to_axis > aperture\n",
    "        angular_violation = torch.relu(d_to_axis - aperture)\n",
    "        \n",
    "        # Combine: if radial violated, use radial + hyperbolic distance\n",
    "        # if radial satisfied, use angular violation\n",
    "        radial_ok = (u_norm < v_norm).float()\n",
    "        \n",
    "        energy = radial_violation + radial_ok * angular_violation\n",
    "        \n",
    "        return energy\n",
    "    \n",
    "    def in_cone(self, u, v):\n",
    "        \"\"\"\n",
    "        Check if v is inside u's shadow cone.\n",
    "        \n",
    "        Returns:\n",
    "            Boolean tensor\n",
    "        \"\"\"\n",
    "        u_norm = self.geom.norm(u)\n",
    "        v_norm = self.geom.norm(v)\n",
    "        cos_sim = self.geom.cosine_similarity(u, v)\n",
    "        \n",
    "        radial_ok = v_norm > u_norm\n",
    "        angular_ok = cos_sim >= self.cos_threshold\n",
    "        \n",
    "        return radial_ok & angular_ok\n",
    "\n",
    "\n",
    "# Test Ball Shadow Cone\n",
    "ball_cone = BallShadowCone(cone_angle_deg=30.0, radial_margin=0.05)\n",
    "\n",
    "# Parent near origin, children at various positions\n",
    "parent = torch.tensor([0.3, 0.0])\n",
    "child_in_cone = torch.tensor([0.6, 0.1])      # Higher norm, aligned\n",
    "child_wrong_angle = torch.tensor([0.1, 0.6])  # Higher norm, wrong angle\n",
    "child_wrong_radius = torch.tensor([0.2, 0.0]) # Lower norm than parent\n",
    "child_at_origin = torch.tensor([0.01, 0.01])  # Near origin\n",
    "\n",
    "print(\"\\nBall Shadow Cone Tests (angle=30°, margin=0.05):\")\n",
    "print(f\"  Parent: norm={ball_cone.geom.norm(parent).item():.3f}\")\n",
    "print(f\"\\n  Child in cone (0.6, 0.1):\")\n",
    "print(f\"    norm={ball_cone.geom.norm(child_in_cone).item():.3f}, \"\n",
    "      f\"energy={ball_cone.cone_energy(parent, child_in_cone).item():.4f}, \"\n",
    "      f\"in_cone={ball_cone.in_cone(parent, child_in_cone).item()}\")\n",
    "print(f\"\\n  Child wrong angle (0.1, 0.6):\")\n",
    "print(f\"    norm={ball_cone.geom.norm(child_wrong_angle).item():.3f}, \"\n",
    "      f\"energy={ball_cone.cone_energy(parent, child_wrong_angle).item():.4f}, \"\n",
    "      f\"in_cone={ball_cone.in_cone(parent, child_wrong_angle).item()}\")\n",
    "print(f\"\\n  Child wrong radius (0.2, 0.0):\")\n",
    "print(f\"    norm={ball_cone.geom.norm(child_wrong_radius).item():.3f}, \"\n",
    "      f\"energy={ball_cone.cone_energy(parent, child_wrong_radius).item():.4f}, \"\n",
    "      f\"in_cone={ball_cone.in_cone(parent, child_wrong_radius).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ball Shadow Cone Encoder and Loss\n",
    "# =============================================================================\n",
    "\n",
    "class BallShadowConeEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode interval pairs to Poincaré Ball with explicit projection.\n",
    "    \n",
    "    Unlike the hypll-based encoder, this one uses a simple tanh projection\n",
    "    to ensure outputs are inside the ball.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=128, output_dim=2, max_norm=0.95):\n",
    "        super().__init__()\n",
    "        self.max_norm = max_norm\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.geom = PoincareBallGeometry(max_norm=max_norm)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Embeddings in Poincaré Ball of shape (batch, output_dim)\n",
    "        \"\"\"\n",
    "        raw = self.network(x)\n",
    "        # Project to ball using smooth tanh projection\n",
    "        return self.geom.project_to_ball(raw)\n",
    "    \n",
    "    def get_embedding(self, start_state, end_state, n_states=6):\n",
    "        \"\"\"Get embedding for a single interval.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            s_norm = start_state / (n_states - 1)\n",
    "            e_norm = end_state / (n_states - 1)\n",
    "            x = torch.tensor([[s_norm, e_norm]], dtype=torch.float32, device=device)\n",
    "            return self.forward(x).squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "class BallShadowConeLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Shadow Cone Loss for Poincaré Ball embeddings.\n",
    "    \n",
    "    Loss = L_positive + μ * L_negative + L_regularization\n",
    "    \n",
    "    - L_positive: Push children into parent cones (higher norm, aligned)\n",
    "    - L_negative: Push incomparable pairs to different cones\n",
    "    - L_regularization: Prevent collapse and encourage spread\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cone_angle_deg=30.0,\n",
    "        radial_margin=0.05,\n",
    "        angular_weight=1.0,\n",
    "        positive_margin=0.0,\n",
    "        negative_margin=0.5,\n",
    "        neg_weight=1.0,\n",
    "        temperature=5.0,\n",
    "        boundary_weight=0.1,\n",
    "        origin_weight=0.01,\n",
    "        spread_weight=0.01,\n",
    "        max_norm=0.95,\n",
    "        min_norm=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cone = BallShadowCone(\n",
    "            cone_angle_deg=cone_angle_deg,\n",
    "            radial_margin=radial_margin,\n",
    "            angular_weight=angular_weight,\n",
    "            max_norm=max_norm,\n",
    "        )\n",
    "        \n",
    "        self.positive_margin = positive_margin\n",
    "        self.negative_margin = negative_margin\n",
    "        self.neg_weight = neg_weight\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Regularization weights\n",
    "        self.boundary_weight = boundary_weight\n",
    "        self.origin_weight = origin_weight\n",
    "        self.spread_weight = spread_weight\n",
    "        self.max_norm = max_norm\n",
    "        self.min_norm = min_norm\n",
    "    \n",
    "    def positive_loss(self, parent_emb, child_emb):\n",
    "        \"\"\"\n",
    "        Loss for positive (containment) pairs.\n",
    "        \n",
    "        Child should be in parent's shadow cone.\n",
    "        L_pos = mean(softplus(β * (E(parent, child) - γ₁)))\n",
    "        \"\"\"\n",
    "        energy = self.cone.cone_energy(parent_emb, child_emb)\n",
    "        \n",
    "        # Softplus loss with temperature\n",
    "        loss = torch.log1p(torch.exp(self.temperature * (energy - self.positive_margin)))\n",
    "        \n",
    "        return loss.mean()\n",
    "    \n",
    "    def negative_loss(self, emb1, emb2):\n",
    "        \"\"\"\n",
    "        Loss for negative (incomparable) pairs.\n",
    "        \n",
    "        Neither should be in the other's cone.\n",
    "        L_neg = mean(ReLU(γ₂ - E(u, v)) + ReLU(γ₂ - E(v, u)))\n",
    "        \"\"\"\n",
    "        # Energy in both directions\n",
    "        energy_12 = self.cone.cone_energy(emb1, emb2)\n",
    "        energy_21 = self.cone.cone_energy(emb2, emb1)\n",
    "        \n",
    "        # Penalize if either energy is too low (meaning one is in the other's cone)\n",
    "        loss_12 = torch.relu(self.negative_margin - energy_12)\n",
    "        loss_21 = torch.relu(self.negative_margin - energy_21)\n",
    "        \n",
    "        return (loss_12 + loss_21).mean()\n",
    "    \n",
    "    def regularization_loss(self, embeddings):\n",
    "        \"\"\"\n",
    "        Regularization to prevent collapse and encourage spread.\n",
    "        \"\"\"\n",
    "        norms = self.cone.geom.norm(embeddings)\n",
    "        \n",
    "        # Boundary penalty: keep away from boundary\n",
    "        boundary_loss = torch.relu(norms - self.max_norm).pow(2).mean()\n",
    "        \n",
    "        # Origin penalty: keep away from origin (prevent collapse)\n",
    "        origin_loss = torch.relu(self.min_norm - norms).pow(2).mean()\n",
    "        \n",
    "        # Spread penalty: encourage variance in norms\n",
    "        if norms.shape[0] > 1:\n",
    "            norm_var = norms.var()\n",
    "            spread_loss = 1.0 / (norm_var + 1e-6)\n",
    "        else:\n",
    "            spread_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "        \n",
    "        return (self.boundary_weight * boundary_loss + \n",
    "                self.origin_weight * origin_loss + \n",
    "                self.spread_weight * spread_loss)\n",
    "    \n",
    "    def forward(self, embeddings, positive_pairs, negative_pairs):\n",
    "        \"\"\"\n",
    "        Compute total loss.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: All interval embeddings (n_intervals, dim)\n",
    "            positive_pairs: (n_pos, 2) with (parent_idx, child_idx)\n",
    "            negative_pairs: (n_neg, 2) with (idx1, idx2) incomparable\n",
    "        \n",
    "        Returns:\n",
    "            Total loss and component dict\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # Positive loss\n",
    "        if positive_pairs.shape[0] > 0:\n",
    "            parent_emb = embeddings[positive_pairs[:, 0]]\n",
    "            child_emb = embeddings[positive_pairs[:, 1]]\n",
    "            pos_loss = self.positive_loss(parent_emb, child_emb)\n",
    "        else:\n",
    "            pos_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "        losses['positive'] = pos_loss\n",
    "        \n",
    "        # Negative loss\n",
    "        if negative_pairs.shape[0] > 0:\n",
    "            emb1 = embeddings[negative_pairs[:, 0]]\n",
    "            emb2 = embeddings[negative_pairs[:, 1]]\n",
    "            neg_loss = self.negative_loss(emb1, emb2)\n",
    "        else:\n",
    "            neg_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "        losses['negative'] = neg_loss\n",
    "        \n",
    "        # Regularization\n",
    "        reg_loss = self.regularization_loss(embeddings)\n",
    "        losses['regularization'] = reg_loss\n",
    "        \n",
    "        # Total\n",
    "        total_loss = pos_loss + self.neg_weight * neg_loss + reg_loss\n",
    "        losses['total'] = total_loss\n",
    "        \n",
    "        return total_loss, losses\n",
    "\n",
    "\n",
    "# Test the loss\n",
    "print(\"Testing Ball Shadow Cone Loss:\")\n",
    "test_encoder = BallShadowConeEncoder(input_dim=2, hidden_dim=64, output_dim=2).to(device)\n",
    "test_loss_fn = BallShadowConeLoss(cone_angle_deg=30.0, radial_margin=0.05)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.tensor([[0.0, 1.0], [0.2, 0.8], [0.4, 0.6], [0.5, 0.5]], dtype=torch.float32, device=device)\n",
    "test_emb = test_encoder(test_input)\n",
    "\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Embedding shape: {test_emb.shape}\")\n",
    "print(f\"  Embedding norms: {[f'{n:.3f}' for n in torch.norm(test_emb, dim=-1).tolist()]}\")\n",
    "\n",
    "# Test loss\n",
    "test_pos = torch.tensor([[0, 1]], dtype=torch.long, device=device)  # 0 is parent of 1\n",
    "test_neg = torch.tensor([[0, 2], [1, 3]], dtype=torch.long, device=device)\n",
    "\n",
    "loss, components = test_loss_fn(test_emb, test_pos, test_neg)\n",
    "print(f\"\\n  Loss components:\")\n",
    "for name, val in components.items():\n",
    "    print(f\"    {name}: {val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Ball Shadow Cone Model\n",
    "# =============================================================================\n",
    "\n",
    "def train_ball_shadow_cone_model(\n",
    "    encoder,\n",
    "    dataset,\n",
    "    num_epochs=150,\n",
    "    lr=0.001,\n",
    "    cone_angle_deg=30.0,\n",
    "    radial_margin=0.05,\n",
    "    positive_margin=0.0,\n",
    "    negative_margin=0.5,\n",
    "    neg_weight=1.0,\n",
    "    temperature=5.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the ball encoder with shadow cone loss.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-5)\n",
    "    \n",
    "    loss_fn = BallShadowConeLoss(\n",
    "        cone_angle_deg=cone_angle_deg,\n",
    "        radial_margin=radial_margin,\n",
    "        positive_margin=positive_margin,\n",
    "        negative_margin=negative_margin,\n",
    "        neg_weight=neg_weight,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=shadow_cone_collate_fn\n",
    "    )\n",
    "    \n",
    "    history = []\n",
    "    encoder.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = {'total': 0, 'positive': 0, 'negative': 0, 'regularization': 0}\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            for intervals, pos_pairs, neg_pairs in zip(\n",
    "                batch['intervals'], batch['positive_pairs'], batch['negative_pairs']\n",
    "            ):\n",
    "                if intervals.shape[0] < 2:\n",
    "                    continue\n",
    "                \n",
    "                intervals = intervals.to(device)\n",
    "                pos_pairs = pos_pairs.to(device)\n",
    "                neg_pairs = neg_pairs.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                embeddings = encoder(intervals)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, components = loss_fn(embeddings, pos_pairs, neg_pairs)\n",
    "                \n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                for key in epoch_losses:\n",
    "                    if key in components:\n",
    "                        val = components[key]\n",
    "                        if isinstance(val, torch.Tensor):\n",
    "                            epoch_losses[key] += val.item()\n",
    "                        else:\n",
    "                            epoch_losses[key] += val\n",
    "                n_batches += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if n_batches > 0:\n",
    "            for key in epoch_losses:\n",
    "                epoch_losses[key] /= n_batches\n",
    "        \n",
    "        history.append(epoch_losses)\n",
    "        \n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "                  f\"total={epoch_losses['total']:.4f}, \"\n",
    "                  f\"pos={epoch_losses['positive']:.4f}, \"\n",
    "                  f\"neg={epoch_losses['negative']:.4f}, \"\n",
    "                  f\"reg={epoch_losses['regularization']:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Train the Ball Shadow Cone model\n",
    "print(\"=\"*70)\n",
    "print(\"Training Ball Shadow Cone Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ball_shadow_encoder = BallShadowConeEncoder(\n",
    "    input_dim=2,\n",
    "    hidden_dim=128,\n",
    "    output_dim=2,\n",
    "    max_norm=0.95\n",
    ").to(device)\n",
    "\n",
    "ball_shadow_history = train_ball_shadow_cone_model(\n",
    "    ball_shadow_encoder,\n",
    "    shadow_dataset,\n",
    "    num_epochs=150,\n",
    "    lr=0.001,\n",
    "    cone_angle_deg=35.0,\n",
    "    radial_margin=0.05,\n",
    "    positive_margin=0.0,\n",
    "    negative_margin=0.3,\n",
    "    neg_weight=1.0,\n",
    "    temperature=5.0,\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "losses_dict = {k: [h[k] for h in ball_shadow_history] for k in ['total', 'positive', 'negative', 'regularization']}\n",
    "\n",
    "for ax, (name, values) in zip(axes, losses_dict.items()):\n",
    "    ax.plot(values)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'{name.capitalize()} Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Ball Shadow Cone Training', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ball_shadow_cone_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization and Analysis: Ball Shadow Cone\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_ball_shadow_cone_embeddings(encoder, mdp, hitting_stats):\n",
    "    \"\"\"\n",
    "    Visualize Ball Shadow Cone embeddings with cone visualization.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = {}\n",
    "    for state in range(mdp.n_states):\n",
    "        if state != mdp.goal_state:\n",
    "            emb = encoder.get_embedding(state, mdp.goal_state, mdp.n_states)\n",
    "            embeddings[state] = emb\n",
    "    \n",
    "    states = sorted(embeddings.keys())\n",
    "    coords = np.array([embeddings[s] for s in states])\n",
    "    means = np.array([hitting_stats[s]['mean'] for s in states])\n",
    "    vars_ = np.array([hitting_stats[s]['var'] for s in states])\n",
    "    norms = np.array([np.linalg.norm(embeddings[s]) for s in states])\n",
    "    \n",
    "    # Plot 1: Colored by mean\n",
    "    ax = axes[0]\n",
    "    plot_poincare_disk(ax, \"Colored by Mean Hitting Time\")\n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=means, cmap='viridis',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Mean Hitting Time')\n",
    "    \n",
    "    for s in states:\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=11, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Plot 2: Colored by variance\n",
    "    ax = axes[1]\n",
    "    plot_poincare_disk(ax, \"Colored by Variance\")\n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=vars_, cmap='plasma',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    plt.colorbar(scatter, ax=ax, label='Variance')\n",
    "    \n",
    "    for s in states:\n",
    "        emb = embeddings[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=11, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Plot 3: Shadow cone visualization\n",
    "    ax = axes[2]\n",
    "    plot_poincare_disk(ax, \"Shadow Cone Structure (from origin)\")\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(states)))\n",
    "    cone_angle = np.radians(35)  # Same as training\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        emb = embeddings[s]\n",
    "        norm = np.linalg.norm(emb)\n",
    "        \n",
    "        # Draw point\n",
    "        ax.scatter([emb[0]], [emb[1]], c=[colors[i]], s=150,\n",
    "                  edgecolors='black', linewidth=2, zorder=5)\n",
    "        \n",
    "        # Draw cone (sector from this point outward)\n",
    "        if norm > 0.05:\n",
    "            angle = np.arctan2(emb[1], emb[0])\n",
    "            # Draw cone as arc\n",
    "            theta1 = angle - cone_angle\n",
    "            theta2 = angle + cone_angle\n",
    "            \n",
    "            # Draw lines from point to boundary\n",
    "            for theta in [theta1, theta2]:\n",
    "                end_x = 0.95 * np.cos(theta)\n",
    "                end_y = 0.95 * np.sin(theta)\n",
    "                ax.plot([emb[0], end_x], [emb[1], end_y], \n",
    "                       color=colors[i], linewidth=1, alpha=0.3)\n",
    "            \n",
    "            # Draw arc at boundary\n",
    "            arc_theta = np.linspace(theta1, theta2, 30)\n",
    "            arc_x = 0.95 * np.cos(arc_theta)\n",
    "            arc_y = 0.95 * np.sin(arc_theta)\n",
    "            ax.plot(arc_x, arc_y, color=colors[i], linewidth=2, alpha=0.5)\n",
    "        \n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(f\"{label}\\n(r={norm:.2f})\", (emb[0], emb[1]), fontsize=9, fontweight='bold',\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ball_shadow_cone_embeddings.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Visualize\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Ball Shadow Cone Embeddings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ball_shadow_embeddings = visualize_ball_shadow_cone_embeddings(ball_shadow_encoder, mdp, hitting_stats)\n",
    "\n",
    "# Analyze\n",
    "print(\"\\nEmbedding Analysis:\")\n",
    "print(f\"{'Pair':<10} {'x':<10} {'y':<10} {'Norm':<10} {'Mean T':<10} {'Var T':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for s in sorted(ball_shadow_embeddings.keys()):\n",
    "    emb = ball_shadow_embeddings[s]\n",
    "    norm = np.linalg.norm(emb)\n",
    "    label = pair_label(s, mdp.goal_state)\n",
    "    print(f\"{label:<10} {emb[0]:<10.4f} {emb[1]:<10.4f} {norm:<10.4f} \"\n",
    "          f\"{hitting_stats[s]['mean']:<10.2f} {hitting_stats[s]['var']:<10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Final Comparison: All Three Approaches\n",
    "# =============================================================================\n",
    "\n",
    "def plot_all_three_comparison(infonce_emb, halfspace_emb, ball_shadow_emb, hitting_stats, mdp):\n",
    "    \"\"\"\n",
    "    Compare all three embedding approaches side by side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    states = sorted(infonce_emb.keys())\n",
    "    vars_ = np.array([hitting_stats[s]['var'] for s in states])\n",
    "    \n",
    "    # 1. InfoNCE (Poincaré Ball)\n",
    "    ax = axes[0]\n",
    "    plot_poincare_disk(ax, \"InfoNCE Loss\\n(Poincaré Ball)\")\n",
    "    coords = np.array([infonce_emb[s] for s in states])\n",
    "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=vars_, cmap='plasma_r',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    for s in states:\n",
    "        emb = infonce_emb[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=10, fontweight='bold',\n",
    "                   xytext=(4, 4), textcoords='offset points')\n",
    "    \n",
    "    # 2. Shadow Cone (Half-Space)\n",
    "    ax = axes[1]\n",
    "    hs_coords = np.array([halfspace_emb[s] for s in states])\n",
    "    y_max = hs_coords[:, 1].max() * 1.3\n",
    "    plot_half_space(ax, \"Shadow Cone Loss\\n(Half-Space)\", y_max)\n",
    "    scatter = ax.scatter(hs_coords[:, 0], hs_coords[:, 1], c=vars_, cmap='plasma_r',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    for s in states:\n",
    "        emb = halfspace_emb[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=10, fontweight='bold',\n",
    "                   xytext=(4, 4), textcoords='offset points')\n",
    "    \n",
    "    # 3. Shadow Cone (Poincaré Ball)\n",
    "    ax = axes[2]\n",
    "    plot_poincare_disk(ax, \"Shadow Cone Loss\\n(Poincaré Ball)\")\n",
    "    ball_coords = np.array([ball_shadow_emb[s] for s in states])\n",
    "    scatter = ax.scatter(ball_coords[:, 0], ball_coords[:, 1], c=vars_, cmap='plasma_r',\n",
    "                        s=150, edgecolors='black', linewidth=2, zorder=5)\n",
    "    for s in states:\n",
    "        emb = ball_shadow_emb[s]\n",
    "        label = pair_label(s, mdp.goal_state)\n",
    "        ax.annotate(label, (emb[0], emb[1]), fontsize=10, fontweight='bold',\n",
    "                   xytext=(4, 4), textcoords='offset points')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(scatter, ax=axes[2], label='Variance (lighter = lower)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_three_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get all embeddings\n",
    "infonce_emb = get_state_embeddings(model, mdp)\n",
    "\n",
    "# Plot comparison\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL COMPARISON: All Three Approaches\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "plot_all_three_comparison(infonce_emb, shadow_embeddings, ball_shadow_embeddings, hitting_stats, mdp)\n",
    "\n",
    "# Compute correlations for all approaches\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, emb_dict in [(\"InfoNCE (Ball)\", infonce_emb), \n",
    "                        (\"Shadow Cone (Half-Space)\", shadow_embeddings),\n",
    "                        (\"Shadow Cone (Ball)\", ball_shadow_embeddings)]:\n",
    "    states = sorted(emb_dict.keys())\n",
    "    embs = np.array([emb_dict[s] for s in states])\n",
    "    vars_ = np.array([hitting_stats[s]['var'] for s in states])\n",
    "    means = np.array([hitting_stats[s]['mean'] for s in states])\n",
    "    \n",
    "    if name == \"Shadow Cone (Half-Space)\":\n",
    "        # Use y-coordinate\n",
    "        key_coord = embs[:, 1]\n",
    "        coord_name = \"y\"\n",
    "    else:\n",
    "        # Use norm\n",
    "        key_coord = np.linalg.norm(embs, axis=1)\n",
    "        coord_name = \"norm\"\n",
    "    \n",
    "    corr_var, p_var = stats.spearmanr(key_coord, vars_)\n",
    "    corr_mean, p_mean = stats.spearmanr(key_coord, means)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {coord_name} vs variance: ρ = {corr_var:+.4f} (p = {p_var:.4f})\")\n",
    "    print(f\"  {coord_name} vs mean:     ρ = {corr_mean:+.4f} (p = {p_mean:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shadow Cone Loss: Summary\n",
    "\n",
    "### Implementation\n",
    "\n",
    "We implemented the **Shadow Cone Loss** in the **Poincaré Half-Space** model:\n",
    "\n",
    "1. **Half-Space Geometry**: Points $(x, y)$ with $y > 0$\n",
    "   - Distance: $d_{\\mathbb{H}}(p, q) = \\text{arcosh}\\left(1 + \\frac{\\|p - q\\|^2}{2 y_p y_q}\\right)$\n",
    "   - Higher $y$ = more \"general\" (parent intervals)\n",
    "   - Lower $y$ = more \"specific\" (child intervals)\n",
    "\n",
    "2. **Shadow Cone**: Each parent point casts a \"shadow cone\" downward\n",
    "   - Children (contained intervals) should fall inside the parent's cone\n",
    "   - Incomparable intervals should be outside each other's cones\n",
    "\n",
    "3. **Loss Function**:\n",
    "   - **Positive loss**: Push children into parent cones\n",
    "   - **Negative loss**: Push incomparable pairs apart  \n",
    "   - **Regularization**: Encourage spread in y-coordinate\n",
    "\n",
    "### Key Differences from InfoNCE\n",
    "\n",
    "| Property | InfoNCE | Shadow Cone |\n",
    "|----------|---------|-------------|\n",
    "| Relation type | Symmetric similarity | Asymmetric containment |\n",
    "| What it encodes | \"Same vs different\" | \"Contains vs doesn't contain\" |\n",
    "| Geometric structure | Clusters | Tree/hierarchy |\n",
    "| Boundary interpretation | Points at boundary are specific | Points near $y=0$ are specific |\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **InfoNCE**: Good for learning similarity-based representations\n",
    "- **Shadow Cone**: Good for learning hierarchical/compositional structure\n",
    "\n",
    "The Shadow Cone approach explicitly encodes the **poset structure** induced by temporal containment, which may be beneficial for:\n",
    "- Hierarchical planning (decomposing goals into subgoals)\n",
    "- Transfer learning (sharing knowledge between related intervals)\n",
    "- Interpretable representations (clear parent-child relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Hyperbolic Embeddings**: We trained a hyperbolic encoder to embed (state, goal) pairs from MDP trajectories using contrastive learning with temporal containment constraints.\n",
    "\n",
    "2. **Hitting Time Conjecture**: We tested whether:\n",
    "   - **Norm** correlates negatively with **variance** in hitting times\n",
    "   - **Angle** correlates with **mean** hitting times\n",
    "\n",
    "3. **GCBC Policies**: We trained goal-conditioned behavioral cloning policies using:\n",
    "   - Raw state representations (one-hot)\n",
    "   - Hyperbolic embeddings\n",
    "\n",
    "4. **Hyperbolic Planning**: We implemented a planning mechanism that:\n",
    "   - Finds subgoals along the radial line through the target embedding\n",
    "   - Uses lowest-norm atomic embeddings as waypoints\n",
    "   - Recursively decomposes the planning problem\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- The hyperbolic space naturally represents the hierarchical structure of goals\n",
    "- Atomic states (s, s) with low norm may represent \"easy-to-reach\" waypoints\n",
    "- The radial line provides a natural direction for planning\n",
    "- This approach could scale to continuous state spaces with learned decoders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
